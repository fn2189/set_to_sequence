{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order matters \n",
    "Modifying code from http://nlp.seas.harvard.edu/2018/04/03/attention.html to implement the architechture from https://arxiv.org/pdf/1511.06391.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "#%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "#from order_matters import Read, Process, Write, ReadProcessWrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual imports\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import pickle\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.backends import cudnn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "#tensorboard\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#my modules\n",
    "from dataset import DigitsDataset, WordsDataset, VideosDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the case of digits reordering, a small multilayer perceptron\n",
    "    implemented as 1d conv. Specifically, if the input is of shape (batch size, set_length, input_dim), conv1d with\n",
    "    1x1 kernel size and F output filters will give us an output shape of (batch size, set_length, F)\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dim: list of sizes of the embedding at the different layers of the MLP encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_dim=1):\n",
    "        super(ReadLinear, self).__init__()\n",
    "        self.dims = [input_dim] + hidden_dims\n",
    "        self.Ws = [nn.Parameter(torch.randn(self.dims[i+1], self.dims[i])) for i in range(len(self.dims)-1)]\n",
    "        self.bs = [nn.Parameter(torch.randn(self.dims[i+1])) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.Ws = [W.to(device) for W in self.Ws]\n",
    "            self.bs = [b.to(device) for b in self.bs]\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x, n_layers=1):\n",
    "        \"\"\"\n",
    "        x is a batch of sets of shape (batch size, input_dim, set_length) to fit the expected shape of conv1d\n",
    "        We loop over the number of layer of the MLP and for each laer we compute the output of the layer with the corresponding W and b\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.permute(0,2,1,3).unsqueeze(-1) #shape (batch size, set_length, input_dim, 1)\n",
    "        \n",
    "        #reducing by using max\n",
    "        x = torch.max(x, dim=2)[0]\n",
    "        #print(f'X shape: {x.shape}')\n",
    "        for i in range(len(self.dims)-1):\n",
    "            \n",
    "            W = self.Ws[i].unsqueeze(0).unsqueeze(0) #final shape (1, 1, input_dim, output_dim)\n",
    "            b = self.bs[i].unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "            #print(f'x size: {x.size()}, W size: {W.size()}, b size: {b.size()}')\n",
    "            x = self.nonlinearity(torch.matmul(W, x)  + b) # shape (batch size, set_length, hidden_dim, 1)\n",
    "            \n",
    "        x = x.squeeze(-1).permute(0,2,1) # shape (batch size, hidden_dim, set_length)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the character level word encoding, a small multilayer perceptron\n",
    "    implemented as 1d conv. Specifically, the input is of shape (batch size, set_length, max_word_length, input_size). \n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: size of the embedding for the consecutive LSTM layers\n",
    "    input_size: character level vocab_size. Default to 26\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims, input_size=26):\n",
    "        super(ReadWordEncoder, self).__init__()\n",
    "        \n",
    "        self.dims = [input_size] + hidden_dims\n",
    "        self.lstms = [nn.LSTM(input_size=self.dims[i], hidden_size=self.dims[i+1], num_layers=1, batch_first=True) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.lstms = [lstm.to(device) for lstm in self.lstms]\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dims[-1], num_layers=1, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is of shape (batch_size, n_set, max_word_length, vocab_size)\n",
    "        we need to loop over the batch size because lstm batch 1st take input (batch, seq_length, vocab_size)\n",
    "        and so for each element of the batch we have batch -> n_set, seq_length -> max_word_length, vocab_size -> vocab_size\n",
    "        \"\"\"\n",
    "        #print(f'X[i,:,:,:] shape: {x[0, :, :, :].size()}')\n",
    "        l = []\n",
    "        for i in range(x.size(0)):\n",
    "            \"\"\"\n",
    "            #h_n = x[i, :, :, :]\n",
    "            outputs = x[i, :, :, :]\n",
    "            for j in range(len(self.dims)-1):\n",
    "                #outputs, (h_n, c_n) =  self.lstms[j](h_n)\n",
    "                outputs, (h_n, c_n) =  self.lstms[j](outputs)\n",
    "            #l.append(h_n)\n",
    "            #print(f'h_n shape: {h_n.size()}')\n",
    "            \"\"\"\n",
    "            \n",
    "            outputs, (h_n, c_n) =  self.lstm(x[i, :, :, :])\n",
    "            \n",
    "            l.append(h_n)\n",
    "        res = torch.cat(l, dim=0).permute(0,2,1) #shape (batch_size, hidden_dim, n_set)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadVideoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the case of digits reordering, a small multilayer perceptron\n",
    "    implemented as 1d conv. Specifically, if the input is of shape (batch size, set_length, input_dim), conv1d with\n",
    "    1x1 kernel size and F output filters will give us an output shape of (batch size, set_length, F)\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dim: list of sizes of the embedding at the different layers of the MLP encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_dim=1):\n",
    "        super(ReadLinear, self).__init__()\n",
    "        self.dims = [input_dim] + hidden_dims\n",
    "        self.Ws = [nn.Parameter(torch.randn(self.dims[i+1], self.dims[i])) for i in range(len(self.dims)-1)]\n",
    "        self.bs = [nn.Parameter(torch.randn(self.dims[i+1])) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.Ws = [W.to(device) for W in self.Ws]\n",
    "            self.bs = [b.to(device) for b in self.bs]\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x, n_layers=1):\n",
    "        \"\"\"\n",
    "        x is a batch of sets of shape (batch size, input_dim, set_length) to fit the expected shape of conv1d\n",
    "        We loop over the number of layer of the MLP and for each laer we compute the output of the layer with the corresponding W and b\n",
    "        \"\"\"\n",
    "        #print(f'X shape: {x.shape}')\n",
    "        x = x.permute(0,2,1).unsqueeze(-1) #shape (batch size, set_length, input_dim, 1)\n",
    "        for i in range(len(self.dims)-1):\n",
    "            \n",
    "            W = self.Ws[i].unsqueeze(0).unsqueeze(0) #final shape (1, 1, input_dim, output_dim)\n",
    "            b = self.bs[i].unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "            #print(f'x size: {x.size()}, W size: {W.size()}, b size: {b.size()}')\n",
    "            x = self.nonlinearity(torch.matmul(W, x)  + b) # shape (batch size, set_length, hidden_dim, 1)\n",
    "            \n",
    "        x = x.squeeze(-1).permute(0,2,1) # shape (batch size, hidden_dim, set_length)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process(nn.Module):\n",
    "    \"\"\"\n",
    "    A Process block from the Order Matters architechture. Implemented via a self attention mechanism where in order \n",
    "    to compute the next state, we run r_t the attention vector as input for the next step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, lstm_steps, batch_size):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(Process, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_steps = lstm_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.lstmcell = nn.LSTMCell(self.input_dim, self.hidden_dim, bias=True)\n",
    "        ##QUESTION: Should these be initialized to the same value for each member of the batch ?\n",
    "        ### TODO: look into how to initialize LSTM state/output\n",
    "        self.i0 = nn.Parameter(torch.zeros(self.input_dim), requires_grad=False)\n",
    "        #self.h_0 = nn.Parameter(torch.randn(self.hidden_dim), requires_grad=False)\n",
    "        self.h_0 = nn.Parameter(torch.zeros(self.hidden_dim), requires_grad=False)\n",
    "        #self.c_0 = nn.Parameter(torch.randn(self.hidden_dim), requires_grad=False)\n",
    "        self.c_0 = nn.Parameter(torch.zeros(self.hidden_dim), requires_grad=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, M, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        c_t is the state the LSTM evolves, aka q_t from the order matters paper\n",
    "        h and c are initialized randomly\n",
    "        the dot product is scaled to avoid it exploding with the embedding dimension\n",
    "        \n",
    "        The out put, q_t_star = (q_t, r_t) is the linear  is projected with a linear layer to the size of the state of the write LSTM, and used as its initial state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        M: the memories tensor or shape ((batch size, hidden_dim, set_length))\n",
    "        \"\"\"\n",
    "        #To account for the last batch that might not have the same length as the rest\n",
    "        batch_size = M.size(0)\n",
    "        i0 = self.i0.unsqueeze(0).expand(batch_size, -1)\n",
    "        h_0 = self.h_0.unsqueeze(0).expand(batch_size, -1)\n",
    "        c_0 = self.c_0.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        for _ in range(self.lstm_steps):\n",
    "            if _ == 0:\n",
    "                h_t_1 = h_0\n",
    "                c_t_1 = c_0\n",
    "                r_t_1 = i0\n",
    "            h_t, c_t = self.lstmcell(r_t_1, (h_t_1, c_t_1))\n",
    "            d_k = h_t.size(-1)\n",
    "            h_t.size(-1)\n",
    "            \n",
    "            #h_t is of shape (batch_size, hidden_dim) so we expand it\n",
    "            #try:\n",
    "            scores = torch.matmul(M.transpose(-2, -1), h_t.unsqueeze(2)) \\\n",
    "                         / math.sqrt(d_k)\n",
    "            #except:\n",
    "            #    print(f'M: {M.transpose(-2, -1).size()}, h_t: {h_t.size()}')\n",
    "            #    raise RuntimeError('Score error')\n",
    "                \n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            p_attn = F.softmax(scores, dim = -1)\n",
    "            if dropout is not None:\n",
    "                p_attn = dropout(p_attn)\n",
    "            r_t_1 = torch.matmul(M, p_attn).squeeze(-1)\n",
    "            #print(f'r_t_1: {r_t_1.size()}')\n",
    "            h_t_1 = h_t\n",
    "            c_t_1 = c_t\n",
    "            \n",
    "        \"\"\"\n",
    "        return (r_t_1, h_t_1)\n",
    "        \"\"\"\n",
    "        return (r_t_1, c_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention model for Pointer-Net taken from https://github.com/shirgur/PointerNet/blob/master/PointerNet.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ctx_dim, \n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Attention\n",
    "        :param int input_dim: Input's dimension\n",
    "        :param int hidden_dim: Number of hidden units in the attention\n",
    "        \"\"\"\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.ctx_dim = ctx_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.context_linear = nn.Conv1d(ctx_dim, hidden_dim, 1, 1)\n",
    "        self.V = nn.Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
    "        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Initialize vector V\n",
    "        nn.init.uniform_(self.V, -1, 1)\n",
    "\n",
    "    def forward(self, input,\n",
    "                context,\n",
    "                mask):\n",
    "        \"\"\"\n",
    "        Attention - Forward-pass\n",
    "        :param Tensor input: Hidden state h (as said in the Pointer's Network paper:  For the LSTM RNNs, \n",
    "        we use the state after the output gate has been component-wise multiplied by the cell activations. #(batch_size, hidden_dim)\n",
    "        \n",
    "        :param Tensor context: Attention context #(batch_size, hidden_dim, seq_len)\n",
    "        :param ByteTensor mask: Selection mask #(batch_size, n_set)\n",
    "        \n",
    "        :return: tuple of - (Attentioned hidden state, Alphas)\n",
    "        \"\"\"\n",
    "\n",
    "        # input is of shape (batch, hidden_dim) so inp will be of shape (batch_size, hidden_dim, seq_len)\n",
    "        inp = self.input_linear(input.unsqueeze(2).transpose(-2, -1)).transpose(-2, -1).repeat(1,1,context.size(-1))\n",
    "\n",
    "        # context is M from the process block shape (batch, input_dim, seq_len)\n",
    "        #so ctx is of shape (batch, hidden_dim, seq_len)\n",
    "        ctx = self.context_linear(context)\n",
    "\n",
    "        # V will of shape (batch, 1, hidden_dim)\n",
    "        V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
    "\n",
    "        # att will be of shape (batch, seq_len)\n",
    "        att = torch.bmm(V, self.tanh(inp + ctx)).squeeze(1)\n",
    "        if len(att[mask]) > 0:\n",
    "            att[mask] = self.inf[mask]\n",
    "        \n",
    "        alpha = self.softmax(att)\n",
    "\n",
    "        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return hidden_state, alpha\n",
    "\n",
    "    def init_inf(self, mask_size):\n",
    "        self.inf = self._inf.unsqueeze(1).expand(*mask_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(nn.Module):\n",
    "    \"\"\"\n",
    "    A Write block from the Order Matters architechture. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Decoder\n",
    "        :param int embedding_dim: Number of embeddings in Pointer-Net\n",
    "        :param int hidden_dim: Number of hidden units for the decoder's RNN\n",
    "        \"\"\"\n",
    "\n",
    "        super(Write, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_to_hidden = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "        self.hidden_out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.att = Attention(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.mask = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.runner = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.lstmcell  = nn.LSTMCell(embedding_dim, hidden_dim, bias=True)\n",
    "    def forward(self, embedded_inputs,\n",
    "                decoder_input,\n",
    "                hidden,\n",
    "                context):\n",
    "        \"\"\"\n",
    "        Decoder - Forward-pass\n",
    "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net #(batch_size, hidden_dim, n_set)\n",
    "        :param Tensor decoder_input: First decoder's input #(batch_size, hidden_dim)\n",
    "        :param Tensor hidden: First decoder's hidden states #((batch_size, hidden_dim),(batch_size, hidden_dim)\n",
    "        :param Tensor context: Encoder's outputs #(batch_size, hidden_dim, n_set)\n",
    "        :return: (Output probabilities, Pointers indices), last hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = embedded_inputs.size(0)\n",
    "        # The size of the set\n",
    "        input_length = embedded_inputs.size(2)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        mask = self.mask.repeat(input_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "        self.att.init_inf(mask.size())\n",
    "\n",
    "        # Generating arang(input_length), broadcasted across batch_size\n",
    "        runner = self.runner.repeat(input_length)\n",
    "        for i in range(input_length):\n",
    "            runner.data[i] = i\n",
    "        runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
    "\n",
    "        outputs = []\n",
    "        pointers = []\n",
    "\n",
    "        def step(x, hidden):\n",
    "            \"\"\"\n",
    "            Recurrence step function\n",
    "            :param Tensor x: Input at time t shape(batch_size, embedding_dim)\n",
    "            :param tuple(Tensor, Tensor) hidden: Hidden states at time t-1\n",
    "            :return: Hidden states at time t (h, c), Attention probabilities (Alpha)\n",
    "            \"\"\"\n",
    "\n",
    "            # Regular LSTM\n",
    "            h, c = hidden #shapes ((batch_size, hidden_dim), (batch_size, hidden_dim))\n",
    "            #print(f'h shape: {h.size()}')\n",
    "            #print(f'x shape: {x.size()}')\n",
    "            \n",
    "            gates = self.input_to_hidden(x) + self.hidden_to_hidden(h.squeeze())\n",
    "            #gates = self.hidden_to_hidden(h.squeeze())\n",
    "            #print(f'gates shape: {gates.size()}')\n",
    "            input, forget, cell, out = gates.chunk(4, 1)\n",
    "\n",
    "            input = torch.sigmoid(input)\n",
    "            forget = torch.sigmoid(forget)\n",
    "            cell = torch.tanh(cell)\n",
    "            out = torch.sigmoid(out)\n",
    "\n",
    "            c_t = (forget * c) + (input * cell)\n",
    "            h_t = out * torch.tanh(c_t)\n",
    "            #print(f'out: {out.size()}, c_t: {c_t.size()}, h_t: {h_t.size()}')\n",
    "\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = torch.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "\n",
    "            return hidden_t, c_t, output\n",
    "        \n",
    "        def step_2(x, hidden):\n",
    "            h, c = hidden\n",
    "            (h_t, c_t) =  self.lstmcell(x, (h, c))\n",
    "            #print('h_t size: ', h_t.size())\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = torch.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "            \n",
    "            return hidden_t, c_t, output\n",
    "\n",
    "        # Recurrence loop\n",
    "        for _ in range(input_length):\n",
    "            #h_t, c_t, outs = step(decoder_input, hidden)\n",
    "            h_t, c_t, outs = step_2(decoder_input, hidden)\n",
    "            hidden = (h_t, c_t)\n",
    "            \n",
    "            # Masking selected inputs\n",
    "            masked_outs = outs * mask\n",
    "\n",
    "            # Get maximum probabilities and indices\n",
    "            max_probs, indices = masked_outs.max(1)\n",
    "            one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()\n",
    "\n",
    "            # Update mask to ignore seen indices\n",
    "            mask  = mask * (1 - one_hot_pointers)\n",
    "\n",
    "            # Get embedded inputs by max indices\n",
    "            embedding_mask = one_hot_pointers.unsqueeze(1).expand(-1, self.embedding_dim, -1).byte()\n",
    "            decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
    "\n",
    "            outputs.append(outs.unsqueeze(0))\n",
    "            pointers.append(indices.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs).permute(1, 0, 2)\n",
    "        pointers = torch.cat(pointers, 1)\n",
    "\n",
    "        return outputs, pointers, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadProcessWrite(nn.Module):\n",
    "    \"\"\"\n",
    "    The full read-process-write from the order matters paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, read_hidden_dims, write_hidden_dim, lstm_steps, batch_size, input_dim=1, reader='linear'):\n",
    "        super(ReadProcessWrite, self).__init__()\n",
    "        self.readers_dict = {'linear': ReadLinear, 'words': ReadWordEncoder, 'videos': ReadLinear}\n",
    "        \n",
    "        #print(f'hidden_dim: {hidden_dim}, input_dim: {input_dim}')\n",
    "        self.decoder_input0 = nn.Parameter(torch.zeros(read_hidden_dims[-1]))\n",
    "        self.decoder_output0 = nn.Parameter(torch.zeros(write_hidden_dim))\n",
    "        self.read = self.readers_dict[reader](read_hidden_dims, input_dim)\n",
    "        self.process = Process(read_hidden_dims[-1], read_hidden_dims[-1], lstm_steps, batch_size)\n",
    "        self.write = Write(read_hidden_dims[-1], write_hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.process_to_write = nn.Linear(read_hidden_dims[-1] * 2, write_hidden_dim) #linear layer to project q_t_star to the hidden size of the write block\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        M = self.read(x)\n",
    "        r_t, c_t = self.process(M)\n",
    "        q_t_star = torch.cat([r_t, c_t], dim=-1) #shape (batch_size, 2*hidden_dim)\n",
    "        #print(f'q_t_star shape: {q_t_star.size()}')\n",
    "        \n",
    "        #We project q_t_star using a linear layer to the hidden size of the write block to be the initial hidden state\n",
    "        write_block_hidden_state_0 = self.process_to_write(q_t_star) #shape (batch_size, hidden_dim)\n",
    "        write_block_output_state_0 = self.decoder_output0.unsqueeze(0).expand(batch_size, -1) #shape (batch_size, hidden_dim)\n",
    "        decoder_input0 = self.decoder_input0.unsqueeze(0).expand(batch_size, -1) #shape (batch_size, hidden_dim)\n",
    "        \n",
    "        #print('decoder_input0: ', decoder_input0)\n",
    "        decoder_hidden0 = (write_block_output_state_0, write_block_hidden_state_0)\n",
    "        outputs, pointers, hidden = self.write(M,\n",
    "                                               decoder_input0,\n",
    "                                               decoder_hidden0,\n",
    "                                                 M)\n",
    "        return outputs, pointers, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_hidden_dims = [256]\n",
    "process_hidden_dim = 128\n",
    "write_hidden_dim = 64\n",
    "lstm_steps = 10\n",
    "batch_size = 128\n",
    "input_dim = 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 5\n",
    "x = torch.rand(batch_size, input_dim, set_size)\n",
    "#if torch.cuda.is_available():\n",
    "#    x = x.cuda()\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpw = ReadProcessWrite(read_hidden_dims, write_hidden_dim, lstm_steps, batch_size, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpw(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args):\n",
    "    print(\"=> creating model\")\n",
    "    model = ReadProcessWrite(args.read_hidden_dims, args.write_hidden_dim, args.lstm_steps, args.batch_size, args.input_dim)\n",
    "    \n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            if args.USE_CUDA:\n",
    "                checkpoint = torch.load(args.resume)\n",
    "            else:\n",
    "                checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            #try:\n",
    "            #    args.best_map = checkpoint['val_map']\n",
    "            #except KeyError as e:\n",
    "            #    args.best_map = None\n",
    "            # print(checkpoint['state_dict'].keys())\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            except RuntimeError as e:\n",
    "                print('Could not load state_dict. Attempting to correct for DataParallel module.* parameter names. This may not be the problem however...')\n",
    "                # This catches the case when the model file was save in DataParallel state\n",
    "                # create new OrderedDict that does not contain `module.`\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['state_dict'].items():\n",
    "                    name = k[7:] # remove `module.`\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                model.load_state_dict(new_state_dict)\n",
    "            # print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "            #       .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weights(weights_indices, parameters, writer, n_iter):\n",
    "    \"\"\"\n",
    "    Adds a current set of weights to the writer\n",
    "    \n",
    "    Parameters\n",
    "    =========\n",
    "    weights_indices: dict of the indices of the weights to \n",
    "    capture for each flattened weight vector\n",
    "    \n",
    "    parameters: list of tuple (name, torch.Tensor parameter vector)\n",
    "    writer: the tensorboadX writer object\n",
    "    n_iter: The iteration at which to save\n",
    "    \"\"\"\n",
    "    weights_data = {}\n",
    "    for name, param in parameters:\n",
    "        if param.requires_grad:\n",
    "            indices = weights_indices[name]\n",
    "            for idx in indices:\n",
    "                weights_data[f'{idx}'] = param.data.flatten()[idx]\n",
    "            writer.add_scalars(f'data/weigths/{name}', weights_data, n_iter)\n",
    "            weights_data = {}\n",
    "                   \n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_list(batch):\n",
    "    #print('type(batch[0])', type(batch[0]))\n",
    "    #print('batch[0]: ', batch[0])\n",
    "    \n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    #print('len(batch): ', len(batch))\n",
    "    \n",
    "    if len(set([x[0].size(0) for x in batch])) > 1:\n",
    "        padded_Xs = pad_sequence([x[0] for x in batch], batch_first=True)\n",
    "        #print(f'padded_batch size: {padded_Xs.size()}')\n",
    "        new_batch  = []\n",
    "        for i in range(len(batch)):\n",
    "            new_batch.append((padded_Xs[i],) + batch[i][1:])\n",
    "        \n",
    "        return default_collate(new_batch)\n",
    "    else:\n",
    "        return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, criterion, optimizer, epoch, writer, args):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Training\n",
    "    running_loss = 0.0\n",
    "    loader_len = len(train_loader)\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        X, Y, additional_dict = data\n",
    "        #print('X: ', X)\n",
    "        # Transfer to GPU\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        X, Y = X.to(device).float(), Y.to(device)\n",
    "        #print(f'X shape: {X.size()}, Y shape: {Y.size()}')\n",
    "        #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "        # Model computations\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, pointers, hidden = model(X)\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "        Y = Y.view(-1)\n",
    "        #print(f'outputs: {outputs.size()}, Y: {Y.size()}')\n",
    "        \n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % args.print_offset == args.print_offset -1:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.6f' %\n",
    "                  (epoch + 1, i + 1, running_loss /args.print_offset ))\n",
    "            #print(f'outputs: {outputs[:15,:]}, Y: {Y[:15]}')\n",
    "            writer.add_scalar('data/losses/train_loss', running_loss/args.print_offset, i + 1 + epoch*loader_len)\n",
    "            write_weights(args.weights_indices, args.parameters, writer, i + 1 + epoch*loader_len)\n",
    "            running_loss = 0\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    avg_val_loss = val(val_loader, model, criterion, epoch)\n",
    "    writer.add_scalar('data/losses/val_loss', running_loss/args.print_offset, (epoch+1)*loader_len)\n",
    "    \n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_loader, model, criterion, epoch=0):\n",
    "\n",
    "    # switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        val_loss = 0.0\n",
    "        for cpt, data in enumerate(val_loader, 0):\n",
    "            X, Y, additional_dict = data\n",
    "\n",
    "            # Transfer to GPU\n",
    "            #local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "            X, Y = X.to(device).float(), Y.to(device)\n",
    "            #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, pointers, hidden = model(X)\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "            Y = Y.view(-1)\n",
    "            loss = criterion(outputs, Y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    #cpt here is the last cpt in the loop, len(validator_generator) -1\n",
    "    print(f'Epoch {epoch + 1} validation loss: {val_loss / (cpt+1)}')\n",
    "\n",
    "    return val_loss / (cpt+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if torch.cuda.is_available():\n",
    "        args.USE_CUDA = True\n",
    "        print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "    else:\n",
    "        args.USE_CUDA = False\n",
    "        \n",
    "        \n",
    "    \n",
    "    with open(args.pickle_file, 'rb') as f:\n",
    "        dict_data = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    runs = glob(args.saveprefix+'/*')\n",
    "    it = len(runs) + 1\n",
    "    writer = SummaryWriter(os.path.join(args.tensorboard_saveprefix, str(it)))\n",
    "    writer.add_text('Metadata', 'Run {} metadata :\\n{}'.format(it, args,))\n",
    "    \n",
    "    dataset_class = DATASET_CLASSES[args.reader]\n",
    "    \n",
    "    train_ds = dataset_class(dict_data['train'])\n",
    "    val_ds = dataset_class(dict_data['val'])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            collate_fn = collate_fn_list,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            collate_fn = collate_fn_list,\n",
    "            num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    model = create_model(args)\n",
    "    \n",
    "    args.weights_indices = {}\n",
    "    args.parameters = list(model.named_parameters())\n",
    "    for name, param in args.parameters:\n",
    "        if param.requires_grad:\n",
    "            size = list(param.data.flatten().size())[0]\n",
    "            args.weights_indices[name] = random.sample(range(size), min(5, size))\n",
    "    \n",
    "    \n",
    "    if args.USE_CUDA:\n",
    "        device = torch.cuda.current_device()\n",
    "        #model.cuda()\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device) \n",
    "        net = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(filter(lambda p: p.requires_grad,\n",
    "                                    model.parameters()),\n",
    "                             lr=args.lr)\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    for ind, epoch in enumerate(range(args.epochs)):\n",
    "        val_loss = train(train_loader, val_loader, model, criterion, optimizer, epoch, writer, args)\n",
    "\n",
    "        \n",
    "        is_best = val_loss > best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, is_best, os.path.join(args.saveprefix, str(it), f'ep_{epoch+1}_map_{best_val_loss:.3}'))\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CLASSES = {'linear': DigitsDataset, 'words': WordsDataset, 'videos': VideosDataset}\n",
    "LETTERS = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_18374_3937_5_2019-06-18_11:45:26.327081.pkl' \n",
    "PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_unpooled.pkl' \n",
    "RESUME = ''\n",
    "BATCH_SIZE = 32\n",
    "READ_HIDDEN_DIMS = [128, 128]\n",
    "WRITE_HIDDEN_DIM = 64\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MOMENTUM = .9\n",
    "NESTEROV = False\n",
    "EPOCHS = 100\n",
    "SAVEPREFIX = '../checkpoints/videos'\n",
    "TENSORBOARD_SAVEPREFIX = '../tensorboard/videos'\n",
    "LSTM_STEPS = 10\n",
    "READER = 'videos'\n",
    "INPUT_DIM = 2048\n",
    "DROPOUT = 0.2\n",
    "WORKERS = 4\n",
    "PRINT_OFFSET = 100\n",
    "\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "else:\n",
    "    USE_CUDA = False\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "ARGS =parser.parse_args(args=[])\n",
    "ARGS.pickle_file = PICKLE_FILE\n",
    "ARGS.saveprefix = SAVEPREFIX\n",
    "ARGS.tensorboard_saveprefix = TENSORBOARD_SAVEPREFIX\n",
    "ARGS.batch_size = BATCH_SIZE\n",
    "ARGS.read_hidden_dims = READ_HIDDEN_DIMS\n",
    "ARGS.write_hidden_dim = WRITE_HIDDEN_DIM\n",
    "ARGS.lr = LR\n",
    "ARGS.weight_decay = WEIGHT_DECAY\n",
    "ARGS.momentum = MOMENTUM\n",
    "ARGS.nesterov = NESTEROV\n",
    "ARGS.epochs = EPOCHS\n",
    "ARGS.lstm_steps = LSTM_STEPS\n",
    "ARGS.input_dim = INPUT_DIM\n",
    "ARGS.reader = READER\n",
    "ARGS.dropout = DROPOUT\n",
    "ARGS.workers = WORKERS\n",
    "ARGS.resume =RESUME\n",
    "ARGS.print_offset = PRINT_OFFSET\n",
    "#ARGS.resume = RESUME\n",
    "#ARGS.USE_CUDA = USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU, 4 devices.\n"
     ]
    }
   ],
   "source": [
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(rpw.named_parameters())\n",
    "for for name, param in rpw.named_parameters():\n",
    "    if param.requires_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "weights_indices = {}\n",
    "l = list(rpw.named_parameters())\n",
    "for name, param in l:\n",
    "    if param.requires_grad:\n",
    "        size = list(param.data.flatten().size())[0]\n",
    "        weights_indices[name] = random.sample(range(size), 5)\n",
    "weights_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weights(weights_indices, parameters, writer)\n",
    "    weights_data = {}\n",
    "    for name, param in parameters:\n",
    "        if param.requires_grad:\n",
    "            indices = weights_indices[name]\n",
    "            for idx in indices:\n",
    "                weights_data[f'{name}.{idx}'] = params.data/flatten()[idx]\n",
    "    writer.add_scalars('data/weights', weights_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.uniform(size=(10000, 5))\n",
    "Y_train = np.sort(X, axis=1)\n",
    "X_val = np.random.uniform(size=(10000, 5))\n",
    "Y_val = np.sort(X, axis=1)\n",
    "Y_train[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {'attributes': None, 'split':{'train': [], 'val': []}}\n",
    "for i in range(X_train.shape[0]):\n",
    "    dict_data['split']['train'].append((X_train[i, :], Y_train[i,:]))\n",
    "    dict_data['split']['val'].append((X_val[i, :], Y_val[i,:]))\n",
    "dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (virtualenv_set2seq)",
   "language": "python",
   "name": "virtualenv_set2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

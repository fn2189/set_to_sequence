{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order matters \n",
    "Implementation of the architechture from https://arxiv.org/pdf/1511.06391.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "#%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "#from order_matters import Read, Process, Write, ReadProcessWrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual imports\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import pickle\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.backends import cudnn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "#tensorboard\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#my modules\n",
    "from dataset import DigitsDataset, WordsDataset, VideosDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the case of digits reordering, a small multilayer perceptron. \n",
    "    Specifically, if the input is of shape (batch size, set_length, input_dim), conv1d with\n",
    "     give us an output shape of (batch size, set_length, hidden_dims[-1])\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: list of sizes of the embedding at the different layers of the MLP encoder\n",
    "    input_dim: the dimension of the inpyut features for each element of the set\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_dim=1):\n",
    "        super(ReadLinear, self).__init__()\n",
    "        self.dims = [input_dim] + hidden_dims\n",
    "        self.Ws = [nn.Parameter(torch.randn(self.dims[i+1], self.dims[i])) for i in range(len(self.dims)-1)]\n",
    "        self.bs = [nn.Parameter(torch.randn(self.dims[i+1])) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.Ws = [W.to(device) for W in self.Ws]\n",
    "            self.bs = [b.to(device) for b in self.bs]\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x, n_layers=1):\n",
    "        \"\"\"\n",
    "        x is a batch of sets of shape (batch size, input_dim, set_length) to fit the expected shape of conv1d\n",
    "        We loop over the number of layer of the MLP and for each laer we compute the output of the layer with the corresponding W and b\n",
    "        \"\"\"\n",
    "        #print(f'X shape: {x.shape}')\n",
    "        x = x.permute(0,2,1).unsqueeze(-1) #shape (batch size, set_length, input_dim, 1)\n",
    "        for i in range(len(self.dims)-1):\n",
    "            \n",
    "            W = self.Ws[i].unsqueeze(0).unsqueeze(0) #final shape (1, 1, input_dim, output_dim)\n",
    "            b = self.bs[i].unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "            #print(f'x size: {x.size()}, W size: {W.size()}, b size: {b.size()}')\n",
    "            x = self.nonlinearity(torch.matmul(W, x)  + b) # shape (batch size, set_length, hidden_dim, 1)\n",
    "            \n",
    "        x = x.squeeze(-1).permute(0,2,1) # shape (batch size, hidden_dim, set_length)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the character level word encoding, a (possibly multi stage) char-level\n",
    "    RNN is applied to each element of each set. Specifically, if the input is of shape (batch size, set_length, max_word_length, input_size),\n",
    "    the output is of shape (batch_size, set_length, hidden_dims[-1]). \n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: size of the embedding for the consecutive LSTM layers\n",
    "    input_size: character level vocab_size. Default to 26\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims, input_size=26):\n",
    "        super(ReadWordEncoder, self).__init__()\n",
    "        \"\"\"\n",
    "        self.dims = [input_size] + hidden_dims\n",
    "        self.lstms = [nn.LSTM(input_size=self.dims[i], hidden_size=self.dims[i+1], num_layers=1, batch_first=True) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.lstms = [lstm.to(device) for lstm in self.lstms]\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dims[-1], num_layers=1, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is of shape (batch_size, n_set, max_word_length, vocab_size)\n",
    "        we need to loop over the batch size because lstm batch 1st take input (batch, seq_length, vocab_size)\n",
    "        and so for each element of the batch we have batch -> n_set, seq_length -> max_word_length, vocab_size -> vocab_size\n",
    "        \"\"\"\n",
    "        #print(f'X[i,:,:,:] shape: {x[0, :, :, :].size()}')\n",
    "        l = []\n",
    "        for i in range(x.size(0)):\n",
    "            \"\"\"\n",
    "            #h_n = x[i, :, :, :]\n",
    "            outputs = x[i, :, :, :]\n",
    "            for j in range(len(self.dims)-1):\n",
    "                #outputs, (h_n, c_n) =  self.lstms[j](h_n)\n",
    "                outputs, (h_n, c_n) =  self.lstms[j](outputs)\n",
    "            #l.append(h_n)\n",
    "            #print(f'h_n shape: {h_n.size()}')\n",
    "            \"\"\"\n",
    "            \n",
    "            outputs, (h_n, c_n) =  self.lstm(x[i, :, :, :])\n",
    "            \n",
    "            \n",
    "            l.append(h_n)\n",
    "        res = torch.cat(l, dim=0).permute(0,2,1) #shape (batch_size, hidden_dim, n_set)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadVideoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the case of video reordering with \n",
    "    each element of is encoded by a matrix of the unpooled feature representations of each \n",
    "    frame is the given video blocks (To train on a dataset were pooling has already been do \n",
    "    to each element of each set of the batch,the ReadLinearEncoder is enough). Specifically, \n",
    "    with an input of shape (batch_size, n_set, max_n_blocks_frames_in_batch, input_dim), \n",
    "    were if element of each set is 0-padded if it has less than max_n_blocks_frames_in_batch frames,\n",
    "    we need to pool accros those frames to get a feature representation each element of each set.\n",
    "    We try max_pooling and lstm-encoding. the result, of shape (batch_size, n_set, input_dim)\n",
    "    is the passed to a perceptron to get an output of shape (batch_size, n_set, hidden_dim[-1])\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: list of sizes of the embedding at the different layers of the MLP encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_dim=1):\n",
    "        super(ReadVideoEncoder, self).__init__()\n",
    "        self.lstm_dim = 512\n",
    "        self.dims = [self.lstm_dim] + hidden_dims\n",
    "        self.Ws = [nn.Parameter(torch.randn(self.dims[i+1], self.dims[i])) for i in range(len(self.dims)-1)]\n",
    "        self.bs = [nn.Parameter(torch.randn(self.dims[i+1])) for i in range(len(self.dims)-1)]\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=self.lstm_dim, num_layers=1, batch_first=True)\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.Ws = [W.to(device) for W in self.Ws]\n",
    "            self.bs = [b.to(device) for b in self.bs]\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x, n_layers=1):\n",
    "        \"\"\"\n",
    "        x is a batch of sets of shape (batch size, input_dim, set_length) to fit the expected shape of conv1d\n",
    "        We loop over the number of layer of the MLP and for each laer we compute the output of the layer with the corresponding W and b\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.permute(0,2,1,3) #shape (batch size, set_length, n_frames, input_dim)\n",
    "        \n",
    "        #################reducing by using max############################\n",
    "        #x = torch.max(x, dim=2)[0]\n",
    "        #print(f'X shape: {x.shape}')\n",
    "        \n",
    "        #################Reducing by using a LSTM########################\n",
    "        l = []\n",
    "        for i in range(x.size(0)):\n",
    "            outputs, (h_n, c_n) =  self.lstm(x[i, :, :, :])\n",
    "            \n",
    "            l.append(h_n)\n",
    "        x = torch.cat(l, dim=0).unsqueeze(-1) #shape (batch_size, set_length, input_dim, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(len(self.dims)-1):\n",
    "            \n",
    "            W = self.Ws[i].unsqueeze(0).unsqueeze(0) #final shape (1, 1, input_dim, output_dim)\n",
    "            b = self.bs[i].unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "            #print(f'x size: {x.size()}, W size: {W.size()}, b size: {b.size()}')\n",
    "            x = self.nonlinearity(torch.matmul(W, x)  + b) # shape (batch size, set_length, hidden_dim, 1)\n",
    "            \n",
    "        x = x.squeeze(-1).permute(0,2,1) # shape (batch size, hidden_dim, set_length)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process(nn.Module):\n",
    "    \"\"\"\n",
    "    A Process block from the Order Matters architechture. Implemented via a self attention mechanism where in order \n",
    "    to compute the next state, we run r_t the attention vector as input for the next step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, lstm_steps, batch_size):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(Process, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_steps = lstm_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.lstmcell = nn.LSTMCell(self.input_dim, self.hidden_dim, bias=True)\n",
    "        ##QUESTION: Should these be initialized to the same value for each member of the batch ?\n",
    "        ### TODO: look into how to initialize LSTM state/output\n",
    "        self.i0 = nn.Parameter(torch.zeros(self.input_dim), requires_grad=False)\n",
    "        #self.h_0 = nn.Parameter(torch.randn(self.hidden_dim), requires_grad=False)\n",
    "        self.h_0 = nn.Parameter(torch.zeros(self.hidden_dim), requires_grad=False)\n",
    "        #self.c_0 = nn.Parameter(torch.randn(self.hidden_dim), requires_grad=False)\n",
    "        self.c_0 = nn.Parameter(torch.zeros(self.hidden_dim), requires_grad=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, M, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        c_t is the state the LSTM evolves, aka q_t from the order matters paper\n",
    "        h and c are initialized randomly\n",
    "        the dot product is scaled to avoid it exploding with the embedding dimension\n",
    "        \n",
    "        The out put, q_t_star = (q_t, r_t) is the linear  is projected with a linear layer to the size of the state of the write LSTM, and used as its initial state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        M: the memories tensor or shape ((batch size, hidden_dim, set_length))\n",
    "        \"\"\"\n",
    "        #To account for the last batch that might not have the same length as the rest\n",
    "        batch_size = M.size(0)\n",
    "        i0 = self.i0.unsqueeze(0).expand(batch_size, -1)\n",
    "        h_0 = self.h_0.unsqueeze(0).expand(batch_size, -1)\n",
    "        c_0 = self.c_0.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        for _ in range(self.lstm_steps):\n",
    "            if _ == 0:\n",
    "                h_t_1 = h_0\n",
    "                c_t_1 = c_0\n",
    "                r_t_1 = i0\n",
    "            h_t, c_t = self.lstmcell(r_t_1, (h_t_1, c_t_1))\n",
    "            d_k = h_t.size(-1)\n",
    "            h_t.size(-1)\n",
    "            \n",
    "            #h_t is of shape (batch_size, hidden_dim) so we expand it\n",
    "            #try:\n",
    "            scores = torch.matmul(M.transpose(-2, -1), h_t.unsqueeze(2)) \\\n",
    "                         / math.sqrt(d_k)\n",
    "            #except:\n",
    "            #    print(f'M: {M.transpose(-2, -1).size()}, h_t: {h_t.size()}')\n",
    "            #    raise RuntimeError('Score error')\n",
    "                \n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            p_attn = F.softmax(scores, dim = -1)\n",
    "            if dropout is not None:\n",
    "                p_attn = dropout(p_attn)\n",
    "            r_t_1 = torch.matmul(M, p_attn).squeeze(-1)\n",
    "            #print(f'r_t_1: {r_t_1.size()}')\n",
    "            h_t_1 = h_t\n",
    "            c_t_1 = c_t\n",
    "            \n",
    "        \"\"\"\n",
    "        return (r_t_1, h_t_1)\n",
    "        \"\"\"\n",
    "        return (r_t_1, c_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention model for Pointer-Net taken from https://github.com/shirgur/PointerNet/blob/master/PointerNet.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ctx_dim, \n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Attention\n",
    "        :param int input_dim: Input's dimension\n",
    "        :param int hidden_dim: Number of hidden units in the attention\n",
    "        \"\"\"\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.ctx_dim = ctx_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.context_linear = nn.Conv1d(ctx_dim, hidden_dim, 1, 1)\n",
    "        self.V = nn.Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
    "        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Initialize vector V\n",
    "        nn.init.uniform_(self.V, -1, 1)\n",
    "\n",
    "    def forward(self, input,\n",
    "                context,\n",
    "                mask):\n",
    "        \"\"\"\n",
    "        Attention - Forward-pass\n",
    "        :param Tensor input: Hidden state h (as said in the Pointer's Network paper:  For the LSTM RNNs, \n",
    "        we use the state after the output gate has been component-wise multiplied by the cell activations. #(batch_size, hidden_dim)\n",
    "        \n",
    "        :param Tensor context: Attention context #(batch_size, hidden_dim, seq_len)\n",
    "        :param ByteTensor mask: Selection mask #(batch_size, n_set)\n",
    "        \n",
    "        :return: tuple of - (Attentioned hidden state, Alphas)\n",
    "        \"\"\"\n",
    "\n",
    "        # input is of shape (batch, hidden_dim) so inp will be of shape (batch_size, hidden_dim, seq_len)\n",
    "        inp = self.input_linear(input.unsqueeze(2).transpose(-2, -1)).transpose(-2, -1).repeat(1,1,context.size(-1))\n",
    "\n",
    "        # context is M from the process block shape (batch, input_dim, seq_len)\n",
    "        #so ctx is of shape (batch, hidden_dim, seq_len)\n",
    "        ctx = self.context_linear(context)\n",
    "\n",
    "        # V will of shape (batch, 1, hidden_dim)\n",
    "        V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
    "\n",
    "        # att will be of shape (batch, seq_len)\n",
    "        att = torch.bmm(V, self.tanh(inp + ctx)).squeeze(1)\n",
    "        if len(att[mask]) > 0:\n",
    "            att[mask] = self.inf[mask]\n",
    "        \n",
    "        alpha = self.softmax(att)\n",
    "\n",
    "        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return hidden_state, alpha\n",
    "\n",
    "    def init_inf(self, mask_size):\n",
    "        self.inf = self._inf.unsqueeze(1).expand(*mask_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(nn.Module):\n",
    "    \"\"\"\n",
    "    A Write block from the Order Matters architechture. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Decoder\n",
    "        :param int embedding_dim: Number of embeddings in Pointer-Net\n",
    "        :param int hidden_dim: Number of hidden units for the decoder's RNN\n",
    "        \"\"\"\n",
    "\n",
    "        super(Write, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_to_hidden = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "        self.hidden_out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.att = Attention(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.mask = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.runner = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.lstmcell  = nn.LSTMCell(embedding_dim, hidden_dim, bias=True)\n",
    "    def forward(self, embedded_inputs,\n",
    "                decoder_input,\n",
    "                hidden,\n",
    "                context):\n",
    "        \"\"\"\n",
    "        Decoder - Forward-pass\n",
    "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net #(batch_size, hidden_dim, n_set)\n",
    "        :param Tensor decoder_input: First decoder's input #(batch_size, hidden_dim)\n",
    "        :param Tensor hidden: First decoder's hidden states #((batch_size, hidden_dim),(batch_size, hidden_dim)\n",
    "        :param Tensor context: Encoder's outputs #(batch_size, hidden_dim, n_set)\n",
    "        :return: (Output probabilities, Pointers indices), last hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = embedded_inputs.size(0)\n",
    "        # The size of the set\n",
    "        input_length = embedded_inputs.size(2)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        mask = self.mask.repeat(input_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "        self.att.init_inf(mask.size())\n",
    "\n",
    "        # Generating arang(input_length), broadcasted across batch_size\n",
    "        runner = self.runner.repeat(input_length)\n",
    "        for i in range(input_length):\n",
    "            runner.data[i] = i\n",
    "        runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
    "\n",
    "        outputs = []\n",
    "        pointers = []\n",
    "\n",
    "        def step(x, hidden):\n",
    "            \"\"\"\n",
    "            Recurrence step function\n",
    "            :param Tensor x: Input at time t shape(batch_size, embedding_dim)\n",
    "            :param tuple(Tensor, Tensor) hidden: Hidden states at time t-1\n",
    "            :return: Hidden states at time t (h, c), Attention probabilities (Alpha)\n",
    "            \"\"\"\n",
    "\n",
    "            # Regular LSTM\n",
    "            h, c = hidden #shapes ((batch_size, hidden_dim), (batch_size, hidden_dim))\n",
    "            #print(f'h shape: {h.size()}')\n",
    "            #print(f'x shape: {x.size()}')\n",
    "            \n",
    "            gates = self.input_to_hidden(x) + self.hidden_to_hidden(h.squeeze())\n",
    "            #gates = self.hidden_to_hidden(h.squeeze())\n",
    "            #print(f'gates shape: {gates.size()}')\n",
    "            input, forget, cell, out = gates.chunk(4, 1)\n",
    "\n",
    "            input = torch.sigmoid(input)\n",
    "            forget = torch.sigmoid(forget)\n",
    "            cell = torch.tanh(cell)\n",
    "            out = torch.sigmoid(out)\n",
    "\n",
    "            c_t = (forget * c) + (input * cell)\n",
    "            h_t = out * torch.tanh(c_t)\n",
    "            #print(f'out: {out.size()}, c_t: {c_t.size()}, h_t: {h_t.size()}')\n",
    "\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = torch.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "\n",
    "            return hidden_t, c_t, output\n",
    "        \n",
    "        def step_2(x, hidden):\n",
    "            h, c = hidden\n",
    "            (h_t, c_t) =  self.lstmcell(x, (h, c))\n",
    "            #print('h_t size: ', h_t.size())\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = torch.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "            \n",
    "            return hidden_t, c_t, output\n",
    "\n",
    "        # Recurrence loop\n",
    "        for _ in range(input_length):\n",
    "            #h_t, c_t, outs = step(decoder_input, hidden)\n",
    "            h_t, c_t, outs = step_2(decoder_input, hidden)\n",
    "            hidden = (h_t, c_t)\n",
    "            \n",
    "            # Masking selected inputs\n",
    "            masked_outs = outs * mask\n",
    "\n",
    "            # Get maximum probabilities and indices\n",
    "            max_probs, indices = masked_outs.max(1)\n",
    "            one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()\n",
    "\n",
    "            # Update mask to ignore seen indices\n",
    "            mask  = mask * (1 - one_hot_pointers)\n",
    "\n",
    "            # Get embedded inputs by max indices\n",
    "            embedding_mask = one_hot_pointers.unsqueeze(1).expand(-1, self.embedding_dim, -1).byte()\n",
    "            decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
    "\n",
    "            outputs.append(outs.unsqueeze(0))\n",
    "            pointers.append(indices.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs).permute(1, 0, 2)\n",
    "        pointers = torch.cat(pointers, 1)\n",
    "\n",
    "        return outputs, pointers, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadProcessWrite(nn.Module):\n",
    "    \"\"\"\n",
    "    The full read-process-write from the order matters paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, read_hidden_dims, write_hidden_dim, lstm_steps, batch_size, input_dim=1, reader='videos'):\n",
    "        super(ReadProcessWrite, self).__init__()\n",
    "        self.readers_dict = {'linear': ReadLinear, 'words': ReadWordEncoder, 'videos': ReadVideoEncoder}\n",
    "        \n",
    "        #print(f'hidden_dim: {hidden_dim}, input_dim: {input_dim}')\n",
    "        self.decoder_input0 = nn.Parameter(torch.zeros(read_hidden_dims[-1]))\n",
    "        self.decoder_output0 = nn.Parameter(torch.zeros(write_hidden_dim))\n",
    "        self.read = self.readers_dict[reader](read_hidden_dims, input_dim)\n",
    "        self.process = Process(read_hidden_dims[-1], read_hidden_dims[-1], lstm_steps, batch_size)\n",
    "        self.write = Write(read_hidden_dims[-1], write_hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.process_to_write = nn.Linear(read_hidden_dims[-1] * 2, write_hidden_dim) #linear layer to project q_t_star to the hidden size of the write block\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        M = self.read(x)\n",
    "        r_t, c_t = self.process(M)\n",
    "        q_t_star = torch.cat([r_t, c_t], dim=-1) #shape (batch_size, 2*hidden_dim)\n",
    "        #print(f'q_t_star shape: {q_t_star.size()}')\n",
    "        \n",
    "        #We project q_t_star using a linear layer to the hidden size of the write block to be the initial hidden state\n",
    "        write_block_hidden_state_0 = self.process_to_write(q_t_star) #shape (batch_size, hidden_dim)\n",
    "        write_block_output_state_0 = self.decoder_output0.unsqueeze(0).expand(batch_size, -1) #shape (batch_size, hidden_dim)\n",
    "        decoder_input0 = self.decoder_input0.unsqueeze(0).expand(batch_size, -1) #shape (batch_size, hidden_dim)\n",
    "        \n",
    "        #print('decoder_input0: ', decoder_input0)\n",
    "        decoder_hidden0 = (write_block_output_state_0, write_block_hidden_state_0)\n",
    "        outputs, pointers, hidden = self.write(M,\n",
    "                                               decoder_input0,\n",
    "                                               decoder_hidden0,\n",
    "                                                 M)\n",
    "        return outputs, pointers, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args):\n",
    "    print(\"=> creating model\")\n",
    "    model = ReadProcessWrite(args.read_hidden_dims, args.write_hidden_dim, args.lstm_steps, args.batch_size, args.input_dim, args.reader)\n",
    "    \n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            if args.USE_CUDA:\n",
    "                checkpoint = torch.load(args.resume)\n",
    "            else:\n",
    "                checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            #try:\n",
    "            #    args.best_map = checkpoint['val_map']\n",
    "            #except KeyError as e:\n",
    "            #    args.best_map = None\n",
    "            # print(checkpoint['state_dict'].keys())\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            except RuntimeError as e:\n",
    "                print('Could not load state_dict. Attempting to correct for DataParallel module.* parameter names. This may not be the problem however...')\n",
    "                # This catches the case when the model file was save in DataParallel state\n",
    "                # create new OrderedDict that does not contain `module.`\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['state_dict'].items():\n",
    "                    name = k[7:] # remove `module.`\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                model.load_state_dict(new_state_dict)\n",
    "            # print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "            #       .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weights(weights_indices, parameters, writer, n_iter):\n",
    "    \"\"\"\n",
    "    Adds a current set of weights to the writer\n",
    "    \n",
    "    Parameters\n",
    "    =========\n",
    "    weights_indices: dict of the indices of the weights to \n",
    "    capture for each flattened weight vector\n",
    "    \n",
    "    parameters: list of tuple (name, torch.Tensor parameter vector)\n",
    "    writer: the tensorboadX writer object\n",
    "    n_iter: The iteration at which to save\n",
    "    \"\"\"\n",
    "    weights_data = {}\n",
    "    for name, param in parameters:\n",
    "        if param.requires_grad:\n",
    "            indices = weights_indices[name]\n",
    "            for idx in indices:\n",
    "                weights_data[f'{idx}'] = param.data.flatten()[idx]\n",
    "            writer.add_scalars(f'data/weigths/{name}', weights_data, n_iter)\n",
    "            weights_data = {}\n",
    "                   \n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_list(batch):\n",
    "    #print('type(batch[0])', type(batch[0]))\n",
    "    #print('batch[0]: ', batch[0])\n",
    "    \n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    #print('len(batch): ', len(batch))\n",
    "    \n",
    "    if len(set([x[0].size(0) for x in batch])) > 1:\n",
    "        padded_Xs = pad_sequence([x[0] for x in batch], batch_first=True)\n",
    "        #print(f'padded_batch size: {padded_Xs.size()}')\n",
    "        new_batch  = []\n",
    "        for i in range(len(batch)):\n",
    "            new_batch.append((padded_Xs[i],) + batch[i][1:])\n",
    "        \n",
    "        return default_collate(new_batch)\n",
    "    else:\n",
    "        return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, criterion, optimizer, epoch, writer, args):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Training\n",
    "    running_loss = 0.0\n",
    "    loader_len = len(train_loader)\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        X, Y, additional_dict = data\n",
    "        #print('X: ', X)\n",
    "        # Transfer to GPU\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        X, Y = X.to(device).float(), Y.to(device)\n",
    "        #print(f'X shape: {X.size()}, Y shape: {Y.size()}')\n",
    "        #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "        # Model computations\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, pointers, hidden = model(X)\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "        Y = Y.view(-1)\n",
    "        #print(f'outputs: {outputs.size()}, Y: {Y.size()}')\n",
    "        \n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % args.print_offset == args.print_offset -1:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.6f' %\n",
    "                  (epoch + 1, i + 1, running_loss /args.print_offset ))\n",
    "            #print(f'outputs: {outputs[:15,:]}, Y: {Y[:15]}')\n",
    "            writer.add_scalar('data/losses/train_loss', running_loss/args.print_offset, i + 1 + epoch*loader_len)\n",
    "            write_weights(args.weights_indices, args.parameters, writer, i + 1 + epoch*loader_len)\n",
    "            running_loss = 0\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    avg_val_loss = val(val_loader, model, criterion, epoch)\n",
    "    writer.add_scalar('data/losses/val_loss', running_loss/args.print_offset, (epoch+1)*loader_len)\n",
    "    \n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_loader, model, criterion, epoch=0):\n",
    "\n",
    "    # switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        val_loss = 0.0\n",
    "        for cpt, data in enumerate(val_loader, 0):\n",
    "            X, Y, additional_dict = data\n",
    "\n",
    "            # Transfer to GPU\n",
    "            #local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "            X, Y = X.to(device).float(), Y.to(device)\n",
    "            #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, pointers, hidden = model(X)\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "            Y = Y.view(-1)\n",
    "            loss = criterion(outputs, Y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    #cpt here is the last cpt in the loop, len(validator_generator) -1\n",
    "    print(f'Epoch {epoch + 1} validation loss: {val_loss / (cpt+1)}')\n",
    "\n",
    "    return val_loss / (cpt+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if torch.cuda.is_available():\n",
    "        args.USE_CUDA = True\n",
    "        print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "    else:\n",
    "        args.USE_CUDA = False\n",
    "        \n",
    "        \n",
    "    \n",
    "    with open(args.pickle_file, 'rb') as f:\n",
    "        dict_data = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    runs = glob(args.saveprefix+'/*')\n",
    "    #ID = args.id\n",
    "    if os.path.exists(os.path.join(args.saveprefix, args.id)):\n",
    "        raise ValueError(f'The specified savepath {os.path.join(args.saveprefix, args.id)} already exists. \\\n",
    "                         check the arguments saveprefix and id')\n",
    "                    \n",
    "    writer = SummaryWriter(os.path.join(args.tensorboard_saveprefix, args.id))\n",
    "    writer.add_text('Metadata', 'Run {} metadata :\\n{}'.format(args.id, args,))\n",
    "    \n",
    "    dataset_class = DATASET_CLASSES[args.reader]\n",
    "    \n",
    "    train_ds = dataset_class(dict_data['train'])\n",
    "    val_ds = dataset_class(dict_data['val'])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            collate_fn = collate_fn_list,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            collate_fn = collate_fn_list,\n",
    "            num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    model = create_model(args)\n",
    "    \n",
    "    args.weights_indices = {}\n",
    "    args.parameters = list(model.named_parameters())\n",
    "    for name, param in args.parameters:\n",
    "        if param.requires_grad:\n",
    "            size = list(param.data.flatten().size())[0]\n",
    "            args.weights_indices[name] = random.sample(range(size), min(5, size))\n",
    "    \n",
    "    \n",
    "    if args.USE_CUDA:\n",
    "        device = torch.cuda.current_device()\n",
    "        #model.cuda()\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device) \n",
    "        net = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(filter(lambda p: p.requires_grad,\n",
    "                                    model.parameters()),\n",
    "                             lr=args.lr)\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    for ind, epoch in enumerate(range(args.epochs)):\n",
    "        val_loss = train(train_loader, val_loader, model, criterion, optimizer, epoch, writer, args)\n",
    "\n",
    "        \n",
    "        is_best = val_loss > best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, is_best, os.path.join(args.saveprefix, args.id, f'ep_{epoch+1}_map_{best_val_loss:.3}'))\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CLASSES = {'linear': DigitsDataset, 'words': WordsDataset, 'videos': VideosDataset}\n",
    "LETTERS = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_18374_3937_5_2019-06-18_11:45:26.327081.pkl' \n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_unpooled.pkl' \n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_10_2019-08-07_12:00:48.139281.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_5_2019-07-27_14:05:17.582365.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_15_2019-08-12_20:04:33.267868.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_30_2019-08-12_21:11:38.404306.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_5_2019-07-28_12:14:08.250888.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_10_2019-08-07_15:04:50.672650.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_15_2019-08-13_15:38:03.834675.pkl'\n",
    "PICKLE_FILE = '../pickles/words_reordering_10000_2000_30_2019-08-13_18:02:03.381022.pkl'\n",
    "RESUME = ''\n",
    "BATCH_SIZE = 32\n",
    "READ_HIDDEN_DIMS = [256]\n",
    "WRITE_HIDDEN_DIM = 256\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "MOMENTUM = .9\n",
    "NESTEROV = False\n",
    "EPOCHS = 100\n",
    "SAVEPREFIX = '../checkpoints/words'\n",
    "TENSORBOARD_SAVEPREFIX = '../tensorboard/words'\n",
    "ID = '77706F30'\n",
    "LSTM_STEPS = 10\n",
    "READER = 'words'\n",
    "INPUT_DIM = 26\n",
    "DROPOUT = 0\n",
    "WORKERS = 4\n",
    "PRINT_OFFSET = 100\n",
    "\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "else:\n",
    "    USE_CUDA = False\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "ARGS =parser.parse_args(args=[])\n",
    "ARGS.pickle_file = PICKLE_FILE\n",
    "ARGS.saveprefix = SAVEPREFIX\n",
    "ARGS.tensorboard_saveprefix = TENSORBOARD_SAVEPREFIX\n",
    "ARGS.batch_size = BATCH_SIZE\n",
    "ARGS.read_hidden_dims = READ_HIDDEN_DIMS\n",
    "ARGS.write_hidden_dim = WRITE_HIDDEN_DIM\n",
    "ARGS.lr = LR\n",
    "ARGS.weight_decay = WEIGHT_DECAY\n",
    "ARGS.momentum = MOMENTUM\n",
    "ARGS.nesterov = NESTEROV\n",
    "ARGS.epochs = EPOCHS\n",
    "ARGS.lstm_steps = LSTM_STEPS\n",
    "ARGS.input_dim = INPUT_DIM\n",
    "ARGS.reader = READER\n",
    "ARGS.dropout = DROPOUT\n",
    "ARGS.workers = WORKERS\n",
    "ARGS.resume =RESUME\n",
    "ARGS.print_offset = PRINT_OFFSET\n",
    "ARGS.id = ID\n",
    "#ARGS.resume = RESUME\n",
    "#ARGS.USE_CUDA = USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "[1,   100] loss: 3.403263\n",
      "[1,   200] loss: 3.403039\n",
      "[1,   300] loss: 3.402820\n",
      "Epoch 1 validation loss: 3.403094965314108\n",
      "[2,   100] loss: 3.402881\n",
      "[2,   200] loss: 3.403218\n",
      "[2,   300] loss: 3.403119\n",
      "Epoch 2 validation loss: 3.4032121650756353\n",
      "[3,   100] loss: 3.403208\n",
      "[3,   200] loss: 3.403426\n",
      "[3,   300] loss: 3.403096\n",
      "Epoch 3 validation loss: 3.403381306027609\n",
      "[4,   100] loss: 3.403427\n",
      "[4,   200] loss: 3.403102\n",
      "[4,   300] loss: 3.403145\n",
      "Epoch 4 validation loss: 3.4032352538335893\n",
      "[5,   100] loss: 3.403375\n",
      "[5,   200] loss: 3.402976\n",
      "[5,   300] loss: 3.403141\n",
      "Epoch 5 validation loss: 3.403288337919447\n",
      "[6,   100] loss: 3.403345\n",
      "[6,   200] loss: 3.402897\n",
      "[6,   300] loss: 3.401136\n",
      "Epoch 6 validation loss: 3.374420847211565\n",
      "[7,   100] loss: 3.361965\n",
      "[7,   200] loss: 3.355138\n",
      "[7,   300] loss: 3.344438\n",
      "Epoch 7 validation loss: 3.336761028047592\n",
      "[8,   100] loss: 3.340470\n",
      "[8,   200] loss: 3.334003\n",
      "[8,   300] loss: 3.324180\n",
      "Epoch 8 validation loss: 3.325855641137986\n",
      "[9,   100] loss: 3.343852\n",
      "[9,   200] loss: 3.342042\n",
      "[9,   300] loss: 3.348258\n",
      "Epoch 9 validation loss: 3.3439863863445463\n",
      "[10,   100] loss: 3.345342\n",
      "[10,   200] loss: 3.348224\n",
      "[10,   300] loss: 3.349311\n",
      "Epoch 10 validation loss: 3.3475543468717546\n",
      "[11,   100] loss: 3.347909\n",
      "[11,   200] loss: 3.346372\n",
      "[11,   300] loss: 3.350712\n",
      "Epoch 11 validation loss: 3.3520459674653553\n",
      "[12,   100] loss: 3.342640\n",
      "[12,   200] loss: 3.352308\n",
      "[12,   300] loss: 3.353509\n",
      "Epoch 12 validation loss: 3.3483489165230402\n",
      "[13,   100] loss: 3.354117\n",
      "[13,   200] loss: 3.345677\n",
      "[13,   300] loss: 3.346884\n",
      "Epoch 13 validation loss: 3.3521906232076977\n",
      "[14,   100] loss: 3.354978\n",
      "[14,   200] loss: 3.342101\n",
      "[14,   300] loss: 3.351466\n",
      "Epoch 14 validation loss: 3.3525186190529475\n",
      "[15,   100] loss: 3.350255\n",
      "[15,   200] loss: 3.353227\n",
      "[15,   300] loss: 3.346321\n",
      "Epoch 15 validation loss: 3.3446487169417125\n",
      "[16,   100] loss: 3.350548\n",
      "[16,   200] loss: 3.353658\n",
      "[16,   300] loss: 3.342412\n",
      "Epoch 16 validation loss: 3.352564020762368\n",
      "[17,   100] loss: 3.352790\n",
      "[17,   200] loss: 3.345408\n",
      "[17,   300] loss: 3.349082\n",
      "Epoch 17 validation loss: 3.3523430332304938\n",
      "[18,   100] loss: 3.351674\n",
      "[18,   200] loss: 3.350975\n",
      "[18,   300] loss: 3.349533\n",
      "Epoch 18 validation loss: 3.3492897124517533\n",
      "[19,   100] loss: 3.349444\n",
      "[19,   200] loss: 3.351506\n",
      "[19,   300] loss: 3.349385\n",
      "Epoch 19 validation loss: 3.353724275316511\n",
      "[20,   100] loss: 3.349969\n",
      "[20,   200] loss: 3.348860\n",
      "[20,   300] loss: 3.350153\n",
      "Epoch 20 validation loss: 3.3507318421015664\n",
      "[21,   100] loss: 3.351489\n",
      "[21,   200] loss: 3.345335\n",
      "[21,   300] loss: 3.350946\n",
      "Epoch 21 validation loss: 3.3532603430369545\n",
      "[22,   100] loss: 3.352502\n",
      "[22,   200] loss: 3.352550\n",
      "[22,   300] loss: 3.346240\n",
      "Epoch 22 validation loss: 3.350530919574556\n",
      "[23,   100] loss: 3.351057\n",
      "[23,   200] loss: 3.347102\n",
      "[23,   300] loss: 3.354634\n",
      "Epoch 23 validation loss: 3.3515018584236267\n",
      "[24,   100] loss: 3.343608\n",
      "[24,   200] loss: 3.357352\n",
      "[24,   300] loss: 3.350690\n",
      "Epoch 24 validation loss: 3.3513439647735113\n",
      "[25,   100] loss: 3.345718\n",
      "[25,   200] loss: 3.349658\n",
      "[25,   300] loss: 3.354919\n",
      "Epoch 25 validation loss: 3.354742924372355\n",
      "[26,   100] loss: 3.345629\n",
      "[26,   200] loss: 3.353040\n",
      "[26,   300] loss: 3.352831\n",
      "Epoch 26 validation loss: 3.3540144950624495\n",
      "[27,   100] loss: 3.351573\n",
      "[27,   200] loss: 3.349276\n",
      "[27,   300] loss: 3.349286\n",
      "Epoch 27 validation loss: 3.3510612760271346\n",
      "[28,   100] loss: 3.350452\n",
      "[28,   200] loss: 3.351012\n",
      "[28,   300] loss: 3.345210\n",
      "Epoch 28 validation loss: 3.353567085568867\n",
      "[29,   100] loss: 3.349264\n",
      "[29,   200] loss: 3.354982\n",
      "[29,   300] loss: 3.345893\n",
      "Epoch 29 validation loss: 3.3539545611729698\n",
      "[30,   100] loss: 3.342029\n",
      "[30,   200] loss: 3.353762\n",
      "[30,   300] loss: 3.350996\n",
      "Epoch 30 validation loss: 3.3528459223489913\n",
      "[31,   100] loss: 3.356457\n",
      "[31,   200] loss: 3.353105\n",
      "[31,   300] loss: 3.340120\n",
      "Epoch 31 validation loss: 3.3504178486173113\n",
      "[32,   100] loss: 3.352264\n",
      "[32,   200] loss: 3.354647\n",
      "[32,   300] loss: 3.345552\n",
      "Epoch 32 validation loss: 3.3531682831900462\n",
      "[33,   100] loss: 3.351029\n",
      "[33,   200] loss: 3.348037\n",
      "[33,   300] loss: 3.351000\n",
      "Epoch 33 validation loss: 3.3530837354205905\n",
      "[34,   100] loss: 3.349914\n",
      "[34,   200] loss: 3.350835\n",
      "[34,   300] loss: 3.350383\n",
      "Epoch 34 validation loss: 3.3547778621552484\n",
      "[35,   100] loss: 3.351141\n",
      "[35,   200] loss: 3.355030\n",
      "[35,   300] loss: 3.349572\n",
      "Epoch 35 validation loss: 3.354888302939279\n",
      "[36,   100] loss: 3.349491\n",
      "[36,   200] loss: 3.351372\n",
      "[36,   300] loss: 3.349202\n",
      "Epoch 36 validation loss: 3.353867829792083\n",
      "[37,   100] loss: 3.347810\n",
      "[37,   200] loss: 3.349140\n",
      "[37,   300] loss: 3.352708\n",
      "Epoch 37 validation loss: 3.3536351786719427\n",
      "[38,   100] loss: 3.351412\n",
      "[38,   200] loss: 3.350590\n",
      "[38,   300] loss: 3.348297\n",
      "Epoch 38 validation loss: 3.3521344623868425\n",
      "[39,   100] loss: 3.355500\n",
      "[39,   200] loss: 3.349372\n",
      "[39,   300] loss: 3.350478\n",
      "Epoch 39 validation loss: 3.3540671136644153\n",
      "[40,   100] loss: 3.353168\n",
      "[40,   200] loss: 3.348161\n",
      "[40,   300] loss: 3.348429\n",
      "Epoch 40 validation loss: 3.3508199131678023\n",
      "[41,   100] loss: 3.353042\n",
      "[41,   200] loss: 3.350049\n",
      "[41,   300] loss: 3.345645\n",
      "Epoch 41 validation loss: 3.3538496494293213\n",
      "[42,   100] loss: 3.347912\n",
      "[42,   200] loss: 3.344792\n",
      "[42,   300] loss: 3.355904\n",
      "Epoch 42 validation loss: 3.353596085593814\n",
      "[43,   100] loss: 3.340213\n",
      "[43,   200] loss: 3.355320\n",
      "[43,   300] loss: 3.354531\n",
      "Epoch 43 validation loss: 3.354923025010124\n",
      "[44,   100] loss: 3.352016\n",
      "[44,   200] loss: 3.357857\n",
      "[44,   300] loss: 3.342617\n",
      "Epoch 44 validation loss: 3.3552659276932006\n",
      "[45,   100] loss: 3.353606\n",
      "[45,   200] loss: 3.347929\n",
      "[45,   300] loss: 3.349368\n",
      "Epoch 45 validation loss: 3.3518258692726257\n",
      "[46,   100] loss: 3.347794\n",
      "[46,   200] loss: 3.351404\n",
      "[46,   300] loss: 3.347943\n",
      "Epoch 46 validation loss: 3.3534201705266558\n",
      "[47,   100] loss: 3.351332\n",
      "[47,   200] loss: 3.349389\n",
      "[47,   300] loss: 3.348538\n",
      "Epoch 47 validation loss: 3.352319554677085\n",
      "[48,   100] loss: 3.350976\n",
      "[48,   200] loss: 3.349760\n",
      "[48,   300] loss: 3.348253\n",
      "Epoch 48 validation loss: 3.353286580433921\n",
      "[49,   100] loss: 3.355430\n",
      "[49,   200] loss: 3.347391\n",
      "[49,   300] loss: 3.347912\n",
      "Epoch 49 validation loss: 3.353209234419323\n",
      "[50,   100] loss: 3.353321\n",
      "[50,   200] loss: 3.349116\n",
      "[50,   300] loss: 3.345972\n",
      "Epoch 50 validation loss: 3.353683494386219\n",
      "[51,   100] loss: 3.347335\n",
      "[51,   200] loss: 3.350585\n",
      "[51,   300] loss: 3.352783\n",
      "Epoch 51 validation loss: 3.3555334673987494\n",
      "[52,   100] loss: 3.346049\n",
      "[52,   200] loss: 3.351786\n",
      "[52,   300] loss: 3.351478\n",
      "Epoch 52 validation loss: 3.355569286951943\n",
      "[53,   100] loss: 3.352364\n",
      "[53,   200] loss: 3.350816\n",
      "[53,   300] loss: 3.349741\n",
      "Epoch 53 validation loss: 3.3505846394432917\n",
      "[54,   100] loss: 3.346009\n",
      "[54,   200] loss: 3.354849\n",
      "[54,   300] loss: 3.347668\n",
      "Epoch 54 validation loss: 3.3535434177943637\n",
      "[55,   100] loss: 3.352476\n",
      "[55,   200] loss: 3.345215\n",
      "[55,   300] loss: 3.354682\n",
      "Epoch 55 validation loss: 3.352577224610344\n",
      "[56,   100] loss: 3.353432\n",
      "[56,   200] loss: 3.348641\n",
      "[56,   300] loss: 3.346431\n",
      "Epoch 56 validation loss: 3.3543850391630143\n",
      "[57,   100] loss: 3.348598\n",
      "[57,   200] loss: 3.353769\n",
      "[57,   300] loss: 3.353416\n",
      "Epoch 57 validation loss: 3.354349492088197\n",
      "[58,   100] loss: 3.348245\n",
      "[58,   200] loss: 3.347741\n",
      "[58,   300] loss: 3.350253\n",
      "Epoch 58 validation loss: 3.3531666445353676\n",
      "[59,   100] loss: 3.349551\n",
      "[59,   200] loss: 3.348748\n",
      "[59,   300] loss: 3.348574\n",
      "Epoch 59 validation loss: 3.3522881515442378\n",
      "[60,   100] loss: 3.348955\n",
      "[60,   200] loss: 3.351436\n",
      "[60,   300] loss: 3.346919\n",
      "Epoch 60 validation loss: 3.3541352256896\n",
      "[61,   100] loss: 3.357105\n",
      "[61,   200] loss: 3.349148\n",
      "[61,   300] loss: 3.345043\n",
      "Epoch 61 validation loss: 3.354101968190027\n",
      "[62,   100] loss: 3.348499\n",
      "[62,   200] loss: 3.347469\n",
      "[62,   300] loss: 3.354577\n",
      "Epoch 62 validation loss: 3.350547960826329\n",
      "[63,   100] loss: 3.348527\n",
      "[63,   200] loss: 3.349550\n",
      "[63,   300] loss: 3.349249\n",
      "Epoch 63 validation loss: 3.3546231474195207\n",
      "[64,   100] loss: 3.349750\n",
      "[64,   200] loss: 3.355856\n",
      "[64,   300] loss: 3.343227\n",
      "Epoch 64 validation loss: 3.350861454766894\n",
      "[65,   100] loss: 3.352690\n",
      "[65,   200] loss: 3.350726\n",
      "[65,   300] loss: 3.348343\n",
      "Epoch 65 validation loss: 3.3547121645912292\n",
      "[66,   100] loss: 3.353602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66,   200] loss: 3.349569\n",
      "[66,   300] loss: 3.348333\n",
      "Epoch 66 validation loss: 3.3493307166629367\n",
      "[67,   100] loss: 3.347718\n",
      "[67,   200] loss: 3.343519\n",
      "[67,   300] loss: 3.355027\n",
      "Epoch 67 validation loss: 3.355143974697779\n",
      "[68,   100] loss: 3.351522\n",
      "[68,   200] loss: 3.350857\n",
      "[68,   300] loss: 3.351843\n",
      "Epoch 68 validation loss: 3.3510691249181352\n",
      "[69,   100] loss: 3.347677\n",
      "[69,   200] loss: 3.354130\n",
      "[69,   300] loss: 3.347814\n",
      "Epoch 69 validation loss: 3.3541552339281355\n",
      "[70,   100] loss: 3.345710\n",
      "[70,   200] loss: 3.350175\n",
      "[70,   300] loss: 3.353107\n",
      "Epoch 70 validation loss: 3.3516319782014876\n",
      "[71,   100] loss: 3.345076\n",
      "[71,   200] loss: 3.350873\n",
      "[71,   300] loss: 3.351411\n",
      "Epoch 71 validation loss: 3.353812017138042\n",
      "[72,   100] loss: 3.353709\n",
      "[72,   200] loss: 3.344751\n",
      "[72,   300] loss: 3.346710\n",
      "Epoch 72 validation loss: 3.351007790792556\n",
      "[73,   100] loss: 3.344252\n",
      "[73,   200] loss: 3.349242\n",
      "[73,   300] loss: 3.351828\n",
      "Epoch 73 validation loss: 3.351780365383814\n",
      "[74,   100] loss: 3.351398\n",
      "[74,   200] loss: 3.342119\n",
      "[74,   300] loss: 3.350785\n",
      "Epoch 74 validation loss: 3.353663800254701\n",
      "[75,   100] loss: 3.346691\n",
      "[75,   200] loss: 3.352291\n",
      "[75,   300] loss: 3.348055\n",
      "Epoch 75 validation loss: 3.3547841518644304\n",
      "[76,   100] loss: 3.352802\n",
      "[76,   200] loss: 3.350766\n",
      "[76,   300] loss: 3.341445\n",
      "Epoch 76 validation loss: 3.3475027841234963\n",
      "[77,   100] loss: 3.349149\n",
      "[77,   200] loss: 3.346268\n",
      "[77,   300] loss: 3.351453\n",
      "Epoch 77 validation loss: 3.353789356019762\n",
      "[78,   100] loss: 3.354636\n",
      "[78,   200] loss: 3.340494\n",
      "[78,   300] loss: 3.355301\n",
      "Epoch 78 validation loss: 3.3550029179406544\n",
      "[79,   100] loss: 3.347969\n",
      "[79,   200] loss: 3.351608\n",
      "[79,   300] loss: 3.352982\n",
      "Epoch 79 validation loss: 3.3443785470629495\n",
      "[80,   100] loss: 3.347169\n",
      "[80,   200] loss: 3.346756\n",
      "[80,   300] loss: 3.355992\n",
      "Epoch 80 validation loss: 3.354329775250147\n",
      "[81,   100] loss: 3.349742\n",
      "[81,   200] loss: 3.353954\n",
      "[81,   300] loss: 3.347089\n",
      "Epoch 81 validation loss: 3.353635806885977\n",
      "[82,   100] loss: 3.354660\n",
      "[82,   200] loss: 3.349457\n",
      "[82,   300] loss: 3.347096\n",
      "Epoch 82 validation loss: 3.3545264100271557\n",
      "[83,   100] loss: 3.354377\n",
      "[83,   200] loss: 3.348350\n",
      "[83,   300] loss: 3.354330\n",
      "Epoch 83 validation loss: 3.354441669252184\n",
      "[84,   100] loss: 3.351517\n",
      "[84,   200] loss: 3.348559\n",
      "[84,   300] loss: 3.348641\n",
      "Epoch 84 validation loss: 3.351482217274015\n",
      "[85,   100] loss: 3.345740\n",
      "[85,   200] loss: 3.353341\n",
      "[85,   300] loss: 3.352206\n",
      "Epoch 85 validation loss: 3.3530115021599665\n",
      "[86,   100] loss: 3.350954\n",
      "[86,   200] loss: 3.353171\n",
      "[86,   300] loss: 3.345246\n",
      "Epoch 86 validation loss: 3.3554884327782526\n",
      "[87,   100] loss: 3.354676\n",
      "[87,   200] loss: 3.352524\n",
      "[87,   300] loss: 3.345601\n",
      "Epoch 87 validation loss: 3.35400114362202\n",
      "[88,   100] loss: 3.350388\n",
      "[88,   200] loss: 3.345823\n",
      "[88,   300] loss: 3.358059\n",
      "Epoch 88 validation loss: 3.355019247721112\n",
      "[89,   100] loss: 3.353315\n",
      "[89,   200] loss: 3.350215\n",
      "[89,   300] loss: 3.350589\n",
      "Epoch 89 validation loss: 3.350497223082043\n",
      "[90,   100] loss: 3.349293\n",
      "[90,   200] loss: 3.351639\n",
      "[90,   300] loss: 3.350222\n",
      "Epoch 90 validation loss: 3.355176392055693\n",
      "[91,   100] loss: 3.349265\n",
      "[91,   200] loss: 3.355212\n",
      "[91,   300] loss: 3.349476\n",
      "Epoch 91 validation loss: 3.3569141531747486\n",
      "[92,   100] loss: 3.351129\n",
      "[92,   200] loss: 3.351688\n",
      "[92,   300] loss: 3.355639\n",
      "Epoch 92 validation loss: 3.3551601455325173\n",
      "[93,   100] loss: 3.353118\n",
      "[93,   200] loss: 3.351459\n",
      "[93,   300] loss: 3.350820\n",
      "Epoch 93 validation loss: 3.355521099908011\n",
      "[94,   100] loss: 3.358140\n",
      "[94,   200] loss: 3.361721\n",
      "[94,   300] loss: 3.340072\n",
      "Epoch 94 validation loss: 3.35498413207039\n",
      "[95,   100] loss: 3.359295\n",
      "[95,   200] loss: 3.342360\n",
      "[95,   300] loss: 3.352592\n",
      "Epoch 95 validation loss: 3.3514661561875116\n",
      "[96,   100] loss: 3.347104\n",
      "[96,   200] loss: 3.345588\n",
      "[96,   300] loss: 3.337922\n",
      "Epoch 96 validation loss: 3.3429730089883956\n",
      "[97,   100] loss: 3.341822\n",
      "[97,   200] loss: 3.338965\n",
      "[97,   300] loss: 3.336464\n",
      "Epoch 97 validation loss: 3.3409452059912303\n",
      "[98,   100] loss: 3.336128\n",
      "[98,   200] loss: 3.339358\n",
      "[98,   300] loss: 3.336364\n",
      "Epoch 98 validation loss: 3.340122771641565\n",
      "[99,   100] loss: 3.335327\n",
      "[99,   200] loss: 3.342989\n",
      "[99,   300] loss: 3.331262\n",
      "Epoch 99 validation loss: 3.3404401219080366\n",
      "[100,   100] loss: 3.345443\n",
      "[100,   200] loss: 3.331356\n",
      "[100,   300] loss: 3.336115\n",
      "Epoch 100 validation loss: 3.3402679269275968\n"
     ]
    }
   ],
   "source": [
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_to_words(X):\n",
    "    \"\"\"\n",
    "    X is of shape (batch, n_seq, max_word_length, vocab_size)\n",
    "    \"\"\"\n",
    "    array = X.data.numpy()\n",
    "    words =  np.ndarray((array.shape[0], array.shape[1]), dtype=object)\n",
    "    words.fill('')\n",
    "    #print(f'Words shape: {words.shape}')\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            for k in range(X.shape[2]):\n",
    "                if max(X[i,j,k,:]) == 1:\n",
    "                    words[i,j] += LETTERS[np.argmax(X[i,j,k,:])]\n",
    "                else:\n",
    "                    pass\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, args):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Training\n",
    "    correct_orders = 0\n",
    "    total_orders = 0\n",
    "    loader_len = len(test_loader)\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        X, Y, additional_dict = data\n",
    "        # Transfer to GPU\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        X, Y = X.to(device).float(), Y.to(device)\n",
    "        #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, pointers, hidden = model(X)\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "        #print(f'outputs: {outputs.size()}, Y: {Y.size()}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        if args.reader == 'words':\n",
    "            words = X_to_words(X.cpu())\n",
    "            #inds_x = np.tile(np.array(range(words.shape[0])), [words.shape[1], 1]).T\n",
    "            predicted_inds = pointers.cpu().data.numpy()\n",
    "            real_inds = Y.cpu().data.numpy()\n",
    "            for i in range(real_inds.shape[0]):\n",
    "                print(f' Predicted Words order: {words[i, predicted_inds[i,:]]}')\n",
    "                print(f' Real Words order: {words[i, real_inds[i,:]]}\\n')\n",
    "            \n",
    "        else :\n",
    "            print(f'Predictions: {pointers}')\n",
    "            print(f'Real orders: {Y}')\n",
    "            \n",
    "        for _ in range(pointers.size(0)):\n",
    "            total_orders += 1\n",
    "            if Y[_,:].equal( pointers[_,:]):\n",
    "                correct_orders +=1\n",
    "                \n",
    "    print(f'Fraction of perfectly sorted sets: {correct_orders/total_orders}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if torch.cuda.is_available():\n",
    "        args.USE_CUDA = True\n",
    "        print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "    else:\n",
    "        args.USE_CUDA = False\n",
    "        \n",
    "        \n",
    "    \n",
    "    with open(args.pickle_file, 'rb') as f:\n",
    "        dict_data = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    #runs = glob(args.saveprefix+'/*')\n",
    "    #it = len(runs) + 1\n",
    "    #writer = SummaryWriter(os.path.join(args.tensorboard_saveprefix, str(it)))\n",
    "    #writer.add_text('Metadata', 'Run {} metadata :\\n{}'.format(it, args,))\n",
    "    \n",
    "    dataset_class = DATASET_CLASSES[args.reader]\n",
    "    \n",
    "    test_ds = dataset_class(dict_data['test'])\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    \n",
    "    model = create_model(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.USE_CUDA:\n",
    "        device = torch.cuda.current_device()\n",
    "        #model.cuda()\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device)\n",
    "        net = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "    test(test_loader, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CLASSES = {'linear': DigitsDataset, 'words': WordsDataset, 'videos': VideosDataset}\n",
    "LETTERS = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_18374_3937_5_2019-06-18_11:45:26.327081.pkl' \n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_unpooled.pkl' \n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_10_2019-08-07_12:00:48.139281.pkl'\n",
    "PICKLE_FILE = '../pickles/digits_reordering_10000_2000_5_2019-07-27_14:05:17.582365.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_15_2019-08-12_20:04:33.267868.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_30_2019-08-12_21:11:38.404306.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_5_2019-07-28_12:14:08.250888.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_10_2019-08-07_15:04:50.672650.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_15_2019-08-13_15:38:03.834675.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_30_2019-08-13_18:02:03.381022.pkl'\n",
    "ID = '77706F30'\n",
    "RESUME = f'../checkpoints/words/{ID}/ep_100_map_inf_latest.pth.tar'\n",
    "BATCH_SIZE = 32\n",
    "READ_HIDDEN_DIMS = [256]\n",
    "WRITE_HIDDEN_DIM = 256\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "MOMENTUM = .9\n",
    "NESTEROV = False\n",
    "LSTM_STEPS = 10\n",
    "READER = 'words'\n",
    "INPUT_DIM = 1\n",
    "DROPOUT = 0\n",
    "WORKERS = 4\n",
    "PRINT_OFFSET = 100\n",
    "\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "else:\n",
    "    USE_CUDA = False\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "ARGS =parser.parse_args(args=[])\n",
    "ARGS.pickle_file = PICKLE_FILE\n",
    "ARGS.batch_size = BATCH_SIZE\n",
    "ARGS.read_hidden_dims = READ_HIDDEN_DIMS\n",
    "ARGS.write_hidden_dim = WRITE_HIDDEN_DIM\n",
    "ARGS.lr = LR\n",
    "ARGS.weight_decay = WEIGHT_DECAY\n",
    "ARGS.momentum = MOMENTUM\n",
    "ARGS.nesterov = NESTEROV\n",
    "ARGS.epochs = EPOCHS\n",
    "ARGS.lstm_steps = LSTM_STEPS\n",
    "ARGS.input_dim = INPUT_DIM\n",
    "ARGS.reader = READER\n",
    "ARGS.dropout = DROPOUT\n",
    "ARGS.workers = WORKERS\n",
    "ARGS.resume =RESUME\n",
    "ARGS.print_offset = PRINT_OFFSET\n",
    "ARGS.id = ID\n",
    "ARGS.resume = RESUME\n",
    "#ARGS.USE_CUDA = USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../evals/words/77706F30/ep_100.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../evals/words/{ID}/ep_100.txt'\n",
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(rpw.named_parameters())\n",
    "for for name, param in rpw.named_parameters():\n",
    "    if param.requires_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "weights_indices = {}\n",
    "l = list(rpw.named_parameters())\n",
    "for name, param in l:\n",
    "    if param.requires_grad:\n",
    "        size = list(param.data.flatten().size())[0]\n",
    "        weights_indices[name] = random.sample(range(size), 5)\n",
    "weights_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weights(weights_indices, parameters, writer)\n",
    "    weights_data = {}\n",
    "    for name, param in parameters:\n",
    "        if param.requires_grad:\n",
    "            indices = weights_indices[name]\n",
    "            for idx in indices:\n",
    "                weights_data[f'{name}.{idx}'] = params.data/flatten()[idx]\n",
    "    writer.add_scalars('data/weights', weights_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.uniform(size=(10000, 5))\n",
    "Y_train = np.sort(X, axis=1)\n",
    "X_val = np.random.uniform(size=(10000, 5))\n",
    "Y_val = np.sort(X, axis=1)\n",
    "Y_train[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {'attributes': None, 'split':{'train': [], 'val': []}}\n",
    "for i in range(X_train.shape[0]):\n",
    "    dict_data['split']['train'].append((X_train[i, :], Y_train[i,:]))\n",
    "    dict_data['split']['val'].append((X_val[i, :], Y_val[i,:]))\n",
    "dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (virtualenv_set2seq)",
   "language": "python",
   "name": "virtualenv_set2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

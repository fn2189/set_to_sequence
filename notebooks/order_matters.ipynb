{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order matters \n",
    "Implementation of the architechture from https://arxiv.org/pdf/1511.06391.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "#from order_matters import Read, Process, Write, ReadProcessWrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual imports\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import pickle\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.backends import cudnn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "#tensorboard\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#my modules\n",
    "from dataset import DigitsDataset, WordsDataset, VideosDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the case of digits reordering, a small multilayer perceptron. \n",
    "    Specifically, if the input is of shape (batch size, set_length, input_dim), conv1d with\n",
    "     give us an output shape of (batch size, set_length, hidden_dims[-1])\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: list of sizes of the embedding at the different layers of the MLP encoder\n",
    "    input_dim: the dimension of the inpyut features for each element of the set\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_dim=1):\n",
    "        super(ReadLinear, self).__init__()\n",
    "        self.dims = [input_dim] + hidden_dims\n",
    "        \"\"\"\n",
    "        self.Ws = [nn.Parameter(torch.randn(self.dims[i+1], self.dims[i])) for i in range(len(self.dims)-1)]\n",
    "        self.bs = [nn.Parameter(torch.randn(self.dims[i+1])) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.Ws = [W.to(device) for W in self.Ws]\n",
    "            self.bs = [b.to(device) for b in self.bs]\n",
    "        \"\"\"\n",
    "        self.linears = self.linears = nn.ModuleList([nn.Linear(self.dims[i], self.dims[i+1]) for i in range(len(self.dims)-1)])\n",
    "        self.nonlinearity = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x, n_layers=1):\n",
    "        \"\"\"\n",
    "        x is a batch of sets of shape (batch size, input_dim, set_length) to fit the expected shape of conv1d\n",
    "        We loop over the number of layer of the MLP and for each laer we compute the output of the layer with the corresponding W and b\n",
    "        \"\"\"\n",
    "        print(f'X shape: {x.shape}')\n",
    "        #x = x.permute(0,2,1)#.unsqueeze(-1) #shape (batch size, set_length, input_dim, 1)\n",
    "        x = x.squeeze().permute(0,2,1) #shape (batch size, set_length, input_dim)\n",
    "        for i in range(len(self.dims)-1):\n",
    "            \"\"\"\n",
    "            W = self.Ws[i].unsqueeze(0).unsqueeze(0) #final shape (1, 1, input_dim, output_dim)\n",
    "            b = self.bs[i].unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "            #print(f'x size: {x.size()}, W size: {W.size()}, b size: {b.size()}')\n",
    "            x = self.nonlinearity(torch.matmul(W, x)  + b) # shape (batch size, set_length, hidden_dim, 1)\n",
    "            \"\"\"\n",
    "            x = self.nonlinearity(self.linears[i](x))\n",
    "        \n",
    "        #print(f'X size: {x.size()}') \n",
    "        x = x.permute(0,2,1) # shape (batch size, hidden_dim, set_length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadLinearVideo(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the case of digits reordering, a small multilayer perceptron. \n",
    "    Specifically, if the input is of shape (batch size, set_length, input_dim), conv1d with\n",
    "     give us an output shape of (batch size, set_length, hidden_dims[-1])\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: list of sizes of the embedding at the different layers of the MLP encoder\n",
    "    input_dim: the dimension of the inpyut features for each element of the set\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_dim=1):\n",
    "        super(ReadLinearVideo, self).__init__()\n",
    "        self.dims = [input_dim] + hidden_dims\n",
    "        \"\"\"\n",
    "        self.Ws = [nn.Parameter(torch.randn(self.dims[i+1], self.dims[i])) for i in range(len(self.dims)-1)]\n",
    "        self.bs = [nn.Parameter(torch.randn(self.dims[i+1])) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.Ws = [W.to(device) for W in self.Ws]\n",
    "            self.bs = [b.to(device) for b in self.bs]\n",
    "        \"\"\"\n",
    "        self.linears = self.linears = nn.ModuleList([nn.Linear(self.dims[i], self.dims[i+1]) for i in range(len(self.dims)-1)])\n",
    "        self.nonlinearity = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x, n_layers=1):\n",
    "        \"\"\"\n",
    "        x is a batch of sets of shape (batch size, input_dim, set_length) to fit the expected shape of conv1d\n",
    "        We loop over the number of layer of the MLP and for each laer we compute the output of the layer with the corresponding W and b\n",
    "        \"\"\"\n",
    "        \n",
    "        #x = x.permute(0,2,1)#.unsqueeze(-1) #shape (batch size, set_length, input_dim, 1)\n",
    "        x = x.squeeze() #shape (batch size, set_length, input_dim)\n",
    "        #print(f'X shape: {x.shape}')\n",
    "        for i in range(len(self.dims)-1):\n",
    "            \"\"\"\n",
    "            W = self.Ws[i].unsqueeze(0).unsqueeze(0) #final shape (1, 1, input_dim, output_dim)\n",
    "            b = self.bs[i].unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "            #print(f'x size: {x.size()}, W size: {W.size()}, b size: {b.size()}')\n",
    "            x = self.nonlinearity(torch.matmul(W, x)  + b) # shape (batch size, set_length, hidden_dim, 1)\n",
    "            \"\"\"\n",
    "            x = self.nonlinearity(self.linears[i](x))\n",
    "        \n",
    "        #print(f'X size: {x.size()}') \n",
    "        x = x.permute(0,2,1) # shape (batch size, hidden_dim, set_length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the character level word encoding, a (possibly multi stage) char-level\n",
    "    RNN is applied to each element of each set. Specifically, if the input is of shape (batch size, set_length, max_word_length, input_size),\n",
    "    the output is of shape (batch_size, set_length, hidden_dims[-1]). \n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: size of the embedding for the consecutive LSTM layers\n",
    "    input_size: character level vocab_size. Default to 26\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims, input_size=26):\n",
    "        super(ReadWordEncoder, self).__init__()\n",
    "        \"\"\"\n",
    "        self.dims = [input_size] + hidden_dims\n",
    "        self.lstms = [nn.LSTM(input_size=self.dims[i], hidden_size=self.dims[i+1], num_layers=1, batch_first=True) for i in range(len(self.dims)-1)]\n",
    "        if torch.cuda.is_available():\n",
    "            device = f'cuda:{torch.cuda.current_device()}' \n",
    "            self.lstms = [lstm.to(device) for lstm in self.lstms]\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dims[-1], num_layers=1, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is of shape (batch_size, n_set, max_word_length, vocab_size)\n",
    "        we need to loop over the batch size because lstm batch 1st take input (batch, seq_length, vocab_size)\n",
    "        and so for each element of the batch we have batch -> n_set, seq_length -> max_word_length, vocab_size -> vocab_size\n",
    "        \"\"\"\n",
    "        #print(f'X[i,:,:,:] shape: {x[0, :, :, :].size()}')\n",
    "        l = []\n",
    "        for i in range(x.size(0)):\n",
    "            \"\"\"\n",
    "            #h_n = x[i, :, :, :]\n",
    "            outputs = x[i, :, :, :]\n",
    "            for j in range(len(self.dims)-1):\n",
    "                #outputs, (h_n, c_n) =  self.lstms[j](h_n)\n",
    "                outputs, (h_n, c_n) =  self.lstms[j](outputs)\n",
    "            #l.append(h_n)\n",
    "            #print(f'h_n shape: {h_n.size()}')\n",
    "            \"\"\"\n",
    "            \n",
    "            outputs, (h_n, c_n) =  self.lstm(x[i, :, :, :])\n",
    "            \n",
    "            \n",
    "            l.append(h_n)\n",
    "        res = torch.cat(l, dim=0).permute(0,2,1) #shape (batch_size, hidden_dim, n_set)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadVideoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A read block from the Order Matters architechture. In the case of video reordering with \n",
    "    each element of is encoded by a matrix of the unpooled feature representations of each \n",
    "    frame is the given video blocks (To train on a dataset were pooling has already been do \n",
    "    to each element of each set of the batch,the ReadLinearEncoder is enough). Specifically, \n",
    "    with an input of shape (batch_size, n_set, max_n_blocks_frames_in_batch, input_dim), \n",
    "    were if element of each set is 0-padded if it has less than max_n_blocks_frames_in_batch frames,\n",
    "    we need to pool accros those frames to get a feature representation each element of each set.\n",
    "    We try max_pooling and lstm-encoding. the result, of shape (batch_size, n_set, input_dim)\n",
    "    is the passed to a perceptron to get an output of shape (batch_size, n_set, hidden_dim[-1])\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    hidden_dims: list of sizes of the embedding at the different layers of the MLP encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_dim=1):\n",
    "        super(ReadVideoEncoder, self).__init__()\n",
    "        self.lstm_dim = 512\n",
    "        #self.dims = [input_dim] + hidden_dims\n",
    "        self.dims = [self.lstm_dims] + hidden_dims\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=self.lstm_dim, num_layers=1, batch_first=True)\n",
    "        self.linears = self.linears = nn.ModuleList([nn.Linear(self.dims[i], self.dims[i+1]) for i in range(len(self.dims)-1)])\n",
    "\n",
    "        self.nonlinearity = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x, n_layers=1):\n",
    "        \"\"\"\n",
    "        x is a batch of sets of shape (batch size, input_dim, set_length) to fit the expected shape of conv1d\n",
    "        We loop over the number of layer of the MLP and for each laer we compute the output of the layer with the corresponding W and b\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.permute(0,2,1,3) #shape (batch size, set_length, n_frames, input_dim)\n",
    "        \n",
    "        #################reducing by using max############################\n",
    "        x = torch.max(x, dim=2)[0]\n",
    "        #print(f'X shape: {x.shape}')\n",
    "        \n",
    "        #################Reducing by using a LSTM########################\n",
    "        \"\"\"\n",
    "        l = []\n",
    "        for i in range(x.size(0)):\n",
    "            outputs, (h_n, c_n) =  self.lstm(x[i, :, :, :])\n",
    "            \n",
    "            l.append(h_n)\n",
    "        x = torch.cat(l, dim=0).unsqueeze(-1) #shape (batch_size, set_length, input_dim, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(len(self.dims)-1):\n",
    "            \"\"\"\n",
    "            W = self.Ws[i].unsqueeze(0).unsqueeze(0) #final shape (1, 1, input_dim, output_dim)\n",
    "            b = self.bs[i].unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "            #print(f'x size: {x.size()}, W size: {W.size()}, b size: {b.size()}')\n",
    "            x = self.nonlinearity(torch.matmul(W, x)  + b) # shape (batch size, set_length, hidden_dim, 1)\n",
    "            \"\"\"\n",
    "            \n",
    "            x = self.nonlinearity(self.linears[i](x))\n",
    "        \n",
    "        #print(f'X size: {x.size()}') \n",
    "        x = x.permute(0,2,1) # shape (batch size, hidden_dim, set_length)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process(nn.Module):\n",
    "    \"\"\"\n",
    "    A Process block from the Order Matters architechture. Implemented via a self attention mechanism where in order \n",
    "    to compute the next state, we run r_t the attention vector as input for the next step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, lstm_steps, batch_size):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(Process, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_steps = lstm_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.lstmcell = nn.LSTMCell(self.input_dim, self.hidden_dim, bias=True)\n",
    "        ##QUESTION: Should these be initialized to the same value for each member of the batch ?\n",
    "        ### TODO: look into how to initialize LSTM state/output\n",
    "        self.i0 = nn.Parameter(torch.zeros(self.input_dim), requires_grad=False)\n",
    "        #self.h_0 = nn.Parameter(torch.randn(self.hidden_dim), requires_grad=False)\n",
    "        self.h_0 = nn.Parameter(torch.zeros(self.hidden_dim), requires_grad=False)\n",
    "        #self.c_0 = nn.Parameter(torch.randn(self.hidden_dim), requires_grad=False)\n",
    "        self.c_0 = nn.Parameter(torch.zeros(self.hidden_dim), requires_grad=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, M, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        c_t is the state the LSTM evolves, aka q_t from the order matters paper\n",
    "        h and c are initialized randomly\n",
    "        the dot product is scaled to avoid it exploding with the embedding dimension\n",
    "        \n",
    "        The out put, q_t_star = (q_t, r_t) is the linear  is projected with a linear layer to the size of the state of the write LSTM, and used as its initial state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        M: the memories tensor or shape ((batch size, hidden_dim, set_length))\n",
    "        \"\"\"\n",
    "        #To account for the last batch that might not have the same length as the rest\n",
    "        batch_size = M.size(0)\n",
    "        i0 = self.i0.unsqueeze(0).expand(batch_size, -1)\n",
    "        h_0 = self.h_0.unsqueeze(0).expand(batch_size, -1)\n",
    "        c_0 = self.c_0.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        #print(f'M shape: {M.shape}')\n",
    "        \n",
    "        for _ in range(self.lstm_steps):\n",
    "            if _ == 0:\n",
    "                h_t_1 = h_0\n",
    "                c_t_1 = c_0\n",
    "                r_t_1 = i0\n",
    "            h_t, c_t = self.lstmcell(r_t_1, (h_t_1, c_t_1))\n",
    "            d_k = h_t.size(-1)\n",
    "            h_t.size(-1)\n",
    "            \n",
    "            #h_t is of shape (batch_size, hidden_dim) so we expand it\n",
    "            try:\n",
    "                scores = torch.matmul(M.transpose(-2, -1), h_t.unsqueeze(2)) \\\n",
    "                             / math.sqrt(d_k)\n",
    "            except:\n",
    "                print(f'M: {M.transpose(-2, -1).size()}, h_t: {h_t.size()}')\n",
    "                raise RuntimeError('Score error')\n",
    "                \n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            p_attn = F.softmax(scores, dim = -1)\n",
    "            if dropout is not None:\n",
    "                p_attn = dropout(p_attn)\n",
    "            r_t_1 = torch.matmul(M, p_attn).squeeze(-1)\n",
    "            #print(f'r_t_1: {r_t_1.size()}')\n",
    "            h_t_1 = h_t\n",
    "            c_t_1 = c_t\n",
    "            \n",
    "        \"\"\"\n",
    "        return (r_t_1, h_t_1)\n",
    "        \"\"\"\n",
    "        return (r_t_1, c_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention model for Pointer-Net taken from https://github.com/shirgur/PointerNet/blob/master/PointerNet.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ctx_dim, \n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Attention\n",
    "        :param int input_dim: Input's dimension\n",
    "        :param int hidden_dim: Number of hidden units in the attention\n",
    "        \"\"\"\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.ctx_dim = ctx_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.context_linear = nn.Conv1d(ctx_dim, hidden_dim, 1, 1)\n",
    "        self.V = nn.Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
    "        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Initialize vector V\n",
    "        nn.init.uniform_(self.V, -1, 1)\n",
    "\n",
    "    def forward(self, input,\n",
    "                context,\n",
    "                mask):\n",
    "        \"\"\"\n",
    "        Attention - Forward-pass\n",
    "        :param Tensor input: Hidden state h (as said in the Pointer's Network paper:  For the LSTM RNNs, \n",
    "        we use the state after the output gate has been component-wise multiplied by the cell activations. #(batch_size, hidden_dim)\n",
    "        \n",
    "        :param Tensor context: Attention context #(batch_size, hidden_dim, seq_len)\n",
    "        :param ByteTensor mask: Selection mask #(batch_size, n_set)\n",
    "        \n",
    "        :return: tuple of - (Attentioned hidden state, Alphas)\n",
    "        \"\"\"\n",
    "\n",
    "        # input is of shape (batch, hidden_dim) so inp will be of shape (batch_size, hidden_dim, seq_len)\n",
    "        inp = self.input_linear(input.unsqueeze(2).transpose(-2, -1)).transpose(-2, -1).repeat(1,1,context.size(-1))\n",
    "\n",
    "        # context is M from the process block shape (batch, input_dim, seq_len)\n",
    "        #so ctx is of shape (batch, hidden_dim, seq_len)\n",
    "        ctx = self.context_linear(context)\n",
    "\n",
    "        # V will of shape (batch, 1, hidden_dim)\n",
    "        V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
    "\n",
    "        # att will be of shape (batch, seq_len)\n",
    "        att = torch.bmm(V, self.tanh(inp + ctx)).squeeze(1)\n",
    "        if len(att[mask]) > 0:\n",
    "            att[mask] = self.inf[mask]\n",
    "        \n",
    "        alpha = self.softmax(att)\n",
    "\n",
    "        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return hidden_state, alpha\n",
    "\n",
    "    def init_inf(self, mask_size):\n",
    "        self.inf = self._inf.unsqueeze(1).expand(*mask_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(nn.Module):\n",
    "    \"\"\"\n",
    "    A Write block from the Order Matters architechture. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Decoder\n",
    "        :param int embedding_dim: Number of embeddings in Pointer-Net\n",
    "        :param int hidden_dim: Number of hidden units for the decoder's RNN\n",
    "        \"\"\"\n",
    "\n",
    "        super(Write, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_to_hidden = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "        self.hidden_out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.att = Attention(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.mask = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.runner = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.lstmcell  = nn.LSTMCell(embedding_dim, hidden_dim, bias=True)\n",
    "    def forward(self, embedded_inputs,\n",
    "                decoder_input,\n",
    "                hidden,\n",
    "                context):\n",
    "        \"\"\"\n",
    "        Decoder - Forward-pass\n",
    "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net #(batch_size, hidden_dim, n_set)\n",
    "        :param Tensor decoder_input: First decoder's input #(batch_size, hidden_dim)\n",
    "        :param Tensor hidden: First decoder's hidden states #((batch_size, hidden_dim),(batch_size, hidden_dim)\n",
    "        :param Tensor context: Encoder's outputs #(batch_size, hidden_dim, n_set)\n",
    "        :return: (Output probabilities, Pointers indices), last hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = embedded_inputs.size(0)\n",
    "        # The size of the set\n",
    "        input_length = embedded_inputs.size(2)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        mask = self.mask.repeat(input_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "        self.att.init_inf(mask.size())\n",
    "\n",
    "        # Generating arang(input_length), broadcasted across batch_size\n",
    "        runner = self.runner.repeat(input_length)\n",
    "        for i in range(input_length):\n",
    "            runner.data[i] = i\n",
    "        runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
    "\n",
    "        outputs = []\n",
    "        pointers = []\n",
    "\n",
    "        def step(x, hidden):\n",
    "            \"\"\"\n",
    "            Recurrence step function\n",
    "            :param Tensor x: Input at time t shape(batch_size, embedding_dim)\n",
    "            :param tuple(Tensor, Tensor) hidden: Hidden states at time t-1\n",
    "            :return: Hidden states at time t (h, c), Attention probabilities (Alpha)\n",
    "            \"\"\"\n",
    "\n",
    "            # Regular LSTM\n",
    "            h, c = hidden #shapes ((batch_size, hidden_dim), (batch_size, hidden_dim))\n",
    "            #print(f'h shape: {h.size()}')\n",
    "            #print(f'x shape: {x.size()}')\n",
    "            \n",
    "            gates = self.input_to_hidden(x) + self.hidden_to_hidden(h.squeeze())\n",
    "            #gates = self.hidden_to_hidden(h.squeeze())\n",
    "            #print(f'gates shape: {gates.size()}')\n",
    "            input, forget, cell, out = gates.chunk(4, 1)\n",
    "\n",
    "            input = torch.sigmoid(input)\n",
    "            forget = torch.sigmoid(forget)\n",
    "            cell = torch.tanh(cell)\n",
    "            out = torch.sigmoid(out)\n",
    "\n",
    "            c_t = (forget * c) + (input * cell)\n",
    "            h_t = out * torch.tanh(c_t)\n",
    "            #print(f'out: {out.size()}, c_t: {c_t.size()}, h_t: {h_t.size()}')\n",
    "\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = torch.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "\n",
    "            return hidden_t, c_t, output\n",
    "        \n",
    "        def step_2(x, hidden):\n",
    "            h, c = hidden\n",
    "            (h_t, c_t) =  self.lstmcell(x, (h, c))\n",
    "            #print('h_t size: ', h_t.size())\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = torch.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "            \n",
    "            return hidden_t, c_t, output\n",
    "\n",
    "        # Recurrence loop\n",
    "        for _ in range(input_length):\n",
    "            #h_t, c_t, outs = step(decoder_input, hidden)\n",
    "            h_t, c_t, outs = step_2(decoder_input, hidden)\n",
    "            hidden = (h_t, c_t)\n",
    "            \n",
    "            # Masking selected inputs\n",
    "            masked_outs = outs * mask\n",
    "\n",
    "            # Get maximum probabilities and indices\n",
    "            max_probs, indices = masked_outs.max(1)\n",
    "            one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()\n",
    "\n",
    "            # Update mask to ignore seen indices\n",
    "            mask  = mask * (1 - one_hot_pointers)\n",
    "\n",
    "            # Get embedded inputs by max indices\n",
    "            embedding_mask = one_hot_pointers.unsqueeze(1).expand(-1, self.embedding_dim, -1).byte()\n",
    "            decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
    "\n",
    "            outputs.append(outs.unsqueeze(0))\n",
    "            pointers.append(indices.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs).permute(1, 0, 2)\n",
    "        pointers = torch.cat(pointers, 1)\n",
    "\n",
    "        return outputs, pointers, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadProcessWrite(nn.Module):\n",
    "    \"\"\"\n",
    "    The full read-process-write from the order matters paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, read_hidden_dims, write_hidden_dim, lstm_steps, batch_size, input_dim=1, reader='videos'):\n",
    "        super(ReadProcessWrite, self).__init__()\n",
    "        #self.readers_dict = {'linear': ReadLinear, 'words': ReadWordEncoder, 'videos': ReadLinearVideo}#ReadVideoEncoder}\n",
    "        self.readers_dict = {'linear': ReadLinear, 'words': ReadWordEncoder, 'videos': ReadVideoEncoder}\n",
    "        \n",
    "        #print(f'hidden_dim: {hidden_dim}, input_dim: {input_dim}')\n",
    "        self.decoder_input0 = nn.Parameter(torch.zeros(read_hidden_dims[-1]))\n",
    "        self.decoder_output0 = nn.Parameter(torch.zeros(write_hidden_dim))\n",
    "        self.read = self.readers_dict[reader](read_hidden_dims, input_dim)\n",
    "        self.process = Process(read_hidden_dims[-1], read_hidden_dims[-1], lstm_steps, batch_size)\n",
    "        self.write = Write(read_hidden_dims[-1], write_hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.process_to_write = nn.Linear(read_hidden_dims[-1] * 2, write_hidden_dim) #linear layer to project q_t_star to the hidden size of the write block\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        M = self.read(x)\n",
    "        r_t, c_t = self.process(M)\n",
    "        q_t_star = torch.cat([r_t, c_t], dim=-1) #shape (batch_size, 2*hidden_dim)\n",
    "        #print(f'q_t_star shape: {q_t_star.size()}')\n",
    "        \n",
    "        #We project q_t_star using a linear layer to the hidden size of the write block to be the initial hidden state\n",
    "        write_block_hidden_state_0 = self.process_to_write(q_t_star) #shape (batch_size, hidden_dim)\n",
    "        write_block_output_state_0 = self.decoder_output0.unsqueeze(0).expand(batch_size, -1) #shape (batch_size, hidden_dim)\n",
    "        decoder_input0 = self.decoder_input0.unsqueeze(0).expand(batch_size, -1) #shape (batch_size, hidden_dim)\n",
    "        \n",
    "        #print('decoder_input0: ', decoder_input0)\n",
    "        decoder_hidden0 = (write_block_output_state_0, write_block_hidden_state_0)\n",
    "        outputs, pointers, hidden = self.write(M,\n",
    "                                               decoder_input0,\n",
    "                                               decoder_hidden0,\n",
    "                                                 M)\n",
    "        return outputs, pointers, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args):\n",
    "    print(\"=> creating model\")\n",
    "    model = ReadProcessWrite(args.read_hidden_dims, args.write_hidden_dim, args.lstm_steps, args.batch_size, args.input_dim, args.reader)\n",
    "    \n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            if args.USE_CUDA:\n",
    "                checkpoint = torch.load(args.resume)\n",
    "            else:\n",
    "                checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            #try:\n",
    "            #    args.best_map = checkpoint['val_map']\n",
    "            #except KeyError as e:\n",
    "            #    args.best_map = None\n",
    "            # print(checkpoint['state_dict'].keys())\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            except RuntimeError as e:\n",
    "                print('Could not load state_dict. Attempting to correct for DataParallel module.* parameter names. This may not be the problem however...')\n",
    "                # This catches the case when the model file was save in DataParallel state\n",
    "                # create new OrderedDict that does not contain `module.`\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['state_dict'].items():\n",
    "                    name = k[7:] # remove `module.`\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                model.load_state_dict(new_state_dict)\n",
    "            # print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "            #       .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weights(weights_indices, parameters, writer, n_iter):\n",
    "    \"\"\"\n",
    "    Adds a current set of weights to the writer\n",
    "    \n",
    "    Parameters\n",
    "    =========\n",
    "    weights_indices: dict of the indices of the weights to \n",
    "    capture for each flattened weight vector\n",
    "    \n",
    "    parameters: list of tuple (name, torch.Tensor parameter vector)\n",
    "    writer: the tensorboadX writer object\n",
    "    n_iter: The iteration at which to save\n",
    "    \"\"\"\n",
    "    weights_data = {}\n",
    "    for name, param in parameters:\n",
    "        if param.requires_grad:\n",
    "            indices = weights_indices[name]\n",
    "            for idx in indices:\n",
    "                weights_data[f'{idx}'] = param.data.flatten()[idx]\n",
    "            writer.add_scalars(f'data/weigths/{name}', weights_data, n_iter)\n",
    "            weights_data = {}\n",
    "                   \n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_list(batch):\n",
    "    #print('type(batch[0])', type(batch[0]))\n",
    "    #print('batch[0]: ', batch[0])\n",
    "    \n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    #print('len(batch): ', len(batch))\n",
    "    \n",
    "    if len(set([x[0].size(0) for x in batch])) > 1:\n",
    "        padded_Xs = pad_sequence([x[0] for x in batch], batch_first=True)\n",
    "        #print(f'padded_batch size: {padded_Xs.size()}')\n",
    "        new_batch  = []\n",
    "        for i in range(len(batch)):\n",
    "            new_batch.append((padded_Xs[i],) + batch[i][1:])\n",
    "        \n",
    "        return default_collate(new_batch)\n",
    "    else:\n",
    "        return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, criterion, optimizer, epoch, writer, args):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Training\n",
    "    running_loss = 0.0\n",
    "    loader_len = len(train_loader)\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        X, Y, additional_dict = data\n",
    "        #print('X: ', X)\n",
    "        # Transfer to GPU\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        X, Y = X.to(device).float(), Y.to(device)\n",
    "        #print(f'X shape: {X.size()}, Y shape: {Y.size()}')\n",
    "        #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "        # Model computations\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, pointers, hidden = model(X)\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "        Y = Y.view(-1)\n",
    "        #print(f'outputs: {outputs.size()}, Y: {Y.size()}')\n",
    "        \n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % args.print_offset == args.print_offset -1:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.6f' %\n",
    "                  (epoch + 1, i + 1, running_loss /args.print_offset ))\n",
    "            #print(f'outputs: {outputs[:15,:]}, Y: {Y[:15]}')\n",
    "            #writer.add_scalar('data/losses/train_loss', running_loss/args.print_offset, i + 1 + epoch*loader_len)\n",
    "            write_weights(args.weights_indices, args.parameters, writer, i + 1 + epoch*loader_len)\n",
    "            #running_loss = 0\n",
    "        \n",
    "\n",
    "    # Validation\n",
    "    avg_val_loss = val(val_loader, model, criterion, epoch)\n",
    "    \n",
    "    #writer.add_scalar('data/losses/train_loss', running_loss/((i+1)*batch_size, (epoch+1)*loader_len)\n",
    "    #writer.add_scalar('data/losses/val_loss', avg_val_loss/args.batch_size, (epoch+1)*loader_len)\n",
    "    losses_data_dict = {}\n",
    "    losses_data_dict['train'] = running_loss/((i+1)*args.batch_size)\n",
    "    losses_data_dict['val'] = avg_val_loss/args.batch_size\n",
    "    writer.add_scalars(f'data/losses', losses_data_dict, epoch+1)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_loader, model, criterion, epoch=0):\n",
    "\n",
    "    # switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        val_loss = 0.0\n",
    "        for cpt, data in enumerate(val_loader, 0):\n",
    "            X, Y, additional_dict = data\n",
    "\n",
    "            # Transfer to GPU\n",
    "            #local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "            X, Y = X.to(device).float(), Y.to(device)\n",
    "            #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, pointers, hidden = model(X)\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "            Y = Y.view(-1)\n",
    "            loss = criterion(outputs, Y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    #cpt here is the last cpt in the loop, len(validator_generator) -1\n",
    "    print(f'Epoch {epoch + 1} validation loss: {val_loss / (cpt+1)}')\n",
    "\n",
    "    return val_loss / (cpt+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if torch.cuda.is_available():\n",
    "        args.USE_CUDA = True\n",
    "        print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "    else:\n",
    "        args.USE_CUDA = False\n",
    "        \n",
    "        \n",
    "    \n",
    "    with open(args.pickle_file, 'rb') as f:\n",
    "        dict_data = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    runs = glob(args.saveprefix+'/*')\n",
    "    #ID = args.id\n",
    "    if os.path.exists(os.path.join(args.saveprefix, args.id)):\n",
    "        raise ValueError(f'The specified savepath {os.path.join(args.saveprefix, args.id)} already exists. \\\n",
    "                         check the arguments saveprefix and id')\n",
    "                    \n",
    "    writer = SummaryWriter(os.path.join(args.tensorboard_saveprefix, args.id))\n",
    "    writer.add_text('Metadata', 'Run {} metadata :\\n{}'.format(args.id, args,))\n",
    "    \n",
    "    dataset_class = DATASET_CLASSES[args.reader]\n",
    "    \n",
    "    train_ds = dataset_class(dict_data['train'])\n",
    "    val_ds = dataset_class(dict_data['val'])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            collate_fn = collate_fn_list,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            collate_fn = collate_fn_list,\n",
    "            num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    model = create_model(args)\n",
    "    \n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv1d, nn.LSTMCell]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    param.data.fill_(0.01)\n",
    "                elif 'weight' in name:    \n",
    "                    torch.nn.init.xavier_uniform(param)\n",
    "            \n",
    "        \n",
    "    #model.apply(init_weights)\n",
    "    \n",
    "    for child in model.children():\n",
    "        print(child)\n",
    "        #for name, param in child.named_parameters():\n",
    "        #    print(name)\n",
    "\n",
    "    \n",
    "    args.weights_indices = {}\n",
    "    args.parameters = list(model.named_parameters())\n",
    "    for name, param in args.parameters:\n",
    "        if param.requires_grad:\n",
    "            size = list(param.data.flatten().size())[0]\n",
    "            args.weights_indices[name] = random.sample(range(size), min(5, size))\n",
    "    \n",
    "    \n",
    "    if args.USE_CUDA:\n",
    "        device = torch.cuda.current_device()\n",
    "        #model.cuda()\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device) \n",
    "        net = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(filter(lambda p: p.requires_grad,\n",
    "                                    model.parameters()),\n",
    "                             lr=args.lr)\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    for ind, epoch in enumerate(range(args.epochs)):\n",
    "        val_loss = train(train_loader, val_loader, model, criterion, optimizer, epoch, writer, args)\n",
    "\n",
    "        \n",
    "        is_best = val_loss > best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, is_best, os.path.join(args.saveprefix, args.id, f'ep_{epoch+1}_map_{best_val_loss:.3}'))\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CLASSES = {'linear': DigitsDataset, 'words': WordsDataset, 'videos': VideosDataset}\n",
    "LETTERS = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_18374_3937_5_2019-06-18_11:45:26.327081.pkl' \n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_resnet50.pkl' \n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_unpooled.pkl' \n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_10_2019-08-07_12:00:48.139281.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_5_2019-07-27_14:05:17.582365.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_15_2019-08-12_20:04:33.267868.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_30_2019-08-12_21:11:38.404306.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_5_2019-07-28_12:14:08.250888.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_10_2019-08-07_15:04:50.672650.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_15_2019-08-13_15:38:03.834675.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_30_2019-08-13_18:02:03.381022.pkl'\n",
    "PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_5.pkl'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_10.pkl'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_15.pkl'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_30.pkl'\n",
    "RESUME = ''\n",
    "BATCH_SIZE = 32\n",
    "READ_HIDDEN_DIMS = [128]\n",
    "WRITE_HIDDEN_DIM = 128\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-6\n",
    "MOMENTUM = .9\n",
    "NESTEROV = False\n",
    "EPOCHS = 100\n",
    "SAVEPREFIX = '../checkpoints/words'\n",
    "TENSORBOARD_SAVEPREFIX = '../tensorboard/words'\n",
    "ID = '5501CC68'\n",
    "LSTM_STEPS = 10\n",
    "READER = 'words'\n",
    "INPUT_DIM = 26\n",
    "DROPOUT = 0.2\n",
    "WORKERS = 4\n",
    "PRINT_OFFSET = 100\n",
    "\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "else:\n",
    "    USE_CUDA = False\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "ARGS =parser.parse_args(args=[])\n",
    "ARGS.pickle_file = PICKLE_FILE\n",
    "ARGS.saveprefix = SAVEPREFIX\n",
    "ARGS.tensorboard_saveprefix = TENSORBOARD_SAVEPREFIX\n",
    "ARGS.batch_size = BATCH_SIZE\n",
    "ARGS.read_hidden_dims = READ_HIDDEN_DIMS\n",
    "ARGS.write_hidden_dim = WRITE_HIDDEN_DIM\n",
    "ARGS.lr = LR\n",
    "ARGS.weight_decay = WEIGHT_DECAY\n",
    "ARGS.momentum = MOMENTUM\n",
    "ARGS.nesterov = NESTEROV\n",
    "ARGS.epochs = EPOCHS\n",
    "ARGS.lstm_steps = LSTM_STEPS\n",
    "ARGS.input_dim = INPUT_DIM\n",
    "ARGS.reader = READER\n",
    "ARGS.dropout = DROPOUT\n",
    "ARGS.workers = WORKERS\n",
    "ARGS.resume =RESUME\n",
    "ARGS.print_offset = PRINT_OFFSET\n",
    "ARGS.id = ID\n",
    "#ARGS.resume = RESUME\n",
    "#ARGS.USE_CUDA = USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "ReadWordEncoder(\n",
      "  (lstm): LSTM(26, 128, batch_first=True)\n",
      ")\n",
      "Process(\n",
      "  (lstmcell): LSTMCell(128, 128)\n",
      ")\n",
      "Write(\n",
      "  (input_to_hidden): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (hidden_to_hidden): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (hidden_out): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (att): Attention(\n",
      "    (input_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (context_linear): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "    (tanh): Tanh()\n",
      "    (softmax): Softmax()\n",
      "  )\n",
      "  (lstmcell): LSTMCell(128, 128)\n",
      ")\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "[1,   100] loss: 1.588509\n",
      "[1,   200] loss: 3.169899\n",
      "[1,   300] loss: 4.933036\n",
      "Epoch 1 validation loss: 1.6481926403348408\n",
      "[2,   100] loss: 1.798293\n",
      "[2,   200] loss: 3.596545\n",
      "[2,   300] loss: 5.346430\n",
      "Epoch 2 validation loss: 1.681536078453064\n",
      "[3,   100] loss: 1.424803\n",
      "[3,   200] loss: 2.821442\n",
      "[3,   300] loss: 4.200118\n",
      "Epoch 3 validation loss: 1.4737037533805484\n",
      "[4,   100] loss: 1.090249\n",
      "[4,   200] loss: 1.998968\n",
      "[4,   300] loss: 2.904987\n",
      "Epoch 4 validation loss: 1.0384684838945903\n",
      "[5,   100] loss: 0.905450\n",
      "[5,   200] loss: 1.810694\n",
      "[5,   300] loss: 2.715825\n",
      "Epoch 5 validation loss: 1.0302293300628662\n",
      "[6,   100] loss: 0.905053\n",
      "[6,   200] loss: 1.810062\n",
      "[6,   300] loss: 2.715038\n",
      "Epoch 6 validation loss: 1.0258454084396362\n",
      "[7,   100] loss: 0.904950\n",
      "[7,   200] loss: 1.809882\n",
      "[7,   300] loss: 2.714799\n",
      "Epoch 7 validation loss: 1.0225278044503832\n",
      "[8,   100] loss: 0.904905\n",
      "[8,   200] loss: 1.809802\n",
      "[8,   300] loss: 2.714691\n",
      "Epoch 8 validation loss: 1.0196654777678231\n",
      "[9,   100] loss: 0.904882\n",
      "[9,   200] loss: 1.809758\n",
      "[9,   300] loss: 2.714631\n",
      "Epoch 9 validation loss: 1.0170409679412842\n",
      "[10,   100] loss: 0.904868\n",
      "[10,   200] loss: 1.809732\n",
      "[10,   300] loss: 2.714594\n",
      "Epoch 10 validation loss: 1.0145566387782021\n",
      "[11,   100] loss: 0.904859\n",
      "[11,   200] loss: 1.809715\n",
      "[11,   300] loss: 2.714570\n",
      "Epoch 11 validation loss: 1.012151831672305\n",
      "[12,   100] loss: 0.904852\n",
      "[12,   200] loss: 1.809703\n",
      "[12,   300] loss: 2.714553\n",
      "Epoch 12 validation loss: 1.0098012666853646\n",
      "[13,   100] loss: 0.904848\n",
      "[13,   200] loss: 1.809695\n",
      "[13,   300] loss: 2.714541\n",
      "Epoch 13 validation loss: 1.007482647895813\n",
      "[14,   100] loss: 0.904845\n",
      "[14,   200] loss: 1.809689\n",
      "[14,   300] loss: 2.714532\n",
      "Epoch 14 validation loss: 1.0051839351654053\n",
      "[15,   100] loss: 0.904842\n",
      "[15,   200] loss: 1.809684\n",
      "[15,   300] loss: 2.714525\n",
      "Epoch 15 validation loss: 1.002899412124876\n",
      "[16,   100] loss: 0.904840\n",
      "[16,   200] loss: 1.809680\n",
      "[16,   300] loss: 2.714520\n",
      "Epoch 16 validation loss: 1.000625727668641\n",
      "[17,   100] loss: 0.904839\n",
      "[17,   200] loss: 1.809677\n",
      "[17,   300] loss: 2.714516\n",
      "Epoch 17 validation loss: 0.9983541359977116\n",
      "[18,   100] loss: 0.904838\n",
      "[18,   200] loss: 1.809675\n",
      "[18,   300] loss: 2.714512\n",
      "Epoch 18 validation loss: 0.9960979213790288\n",
      "[19,   100] loss: 0.904837\n",
      "[19,   200] loss: 1.809674\n",
      "[19,   300] loss: 2.714510\n",
      "Epoch 19 validation loss: 0.9938505802835736\n",
      "[20,   100] loss: 0.904836\n",
      "[20,   200] loss: 1.809672\n",
      "[20,   300] loss: 2.714507\n",
      "Epoch 20 validation loss: 0.9916149453511314\n",
      "[21,   100] loss: 0.904835\n",
      "[21,   200] loss: 1.809671\n",
      "[21,   300] loss: 2.714506\n",
      "Epoch 21 validation loss: 0.9893893069691129\n",
      "[22,   100] loss: 0.904835\n",
      "[22,   200] loss: 1.809670\n",
      "[22,   300] loss: 2.714504\n",
      "Epoch 22 validation loss: 0.9871900687142025\n",
      "[23,   100] loss: 0.904834\n",
      "[23,   200] loss: 1.809668\n",
      "[23,   300] loss: 2.714503\n",
      "Epoch 23 validation loss: 0.9849761137886653\n",
      "[24,   100] loss: 0.904834\n",
      "[24,   200] loss: 1.809668\n",
      "[24,   300] loss: 2.714502\n",
      "Epoch 24 validation loss: 0.982766622588748\n",
      "[25,   100] loss: 0.904834\n",
      "[25,   200] loss: 1.809668\n",
      "[25,   300] loss: 2.714502\n",
      "Epoch 25 validation loss: 0.9806100716666569\n",
      "[26,   100] loss: 0.904834\n",
      "[26,   200] loss: 1.809668\n",
      "[26,   300] loss: 2.714502\n",
      "Epoch 26 validation loss: 0.978347781158629\n",
      "[27,   100] loss: 0.904834\n",
      "[27,   200] loss: 1.809667\n",
      "[27,   300] loss: 2.714501\n",
      "Epoch 27 validation loss: 0.9765164170946393\n",
      "[28,   100] loss: 0.904834\n",
      "[28,   200] loss: 1.809667\n",
      "[28,   300] loss: 2.714501\n",
      "Epoch 28 validation loss: 0.9746264219284058\n",
      "[29,   100] loss: 0.904834\n",
      "[29,   200] loss: 1.809667\n",
      "[29,   300] loss: 2.714500\n",
      "Epoch 29 validation loss: 0.9725983748360286\n",
      "[30,   100] loss: 0.904833\n",
      "[30,   200] loss: 1.809666\n",
      "[30,   300] loss: 2.714499\n",
      "Epoch 30 validation loss: 0.9703871040117173\n",
      "[31,   100] loss: 0.904833\n",
      "[31,   200] loss: 1.809666\n",
      "[31,   300] loss: 2.714499\n",
      "Epoch 31 validation loss: 0.969085815406981\n",
      "[32,   100] loss: 0.904833\n",
      "[32,   200] loss: 1.809666\n",
      "[32,   300] loss: 2.714499\n",
      "Epoch 32 validation loss: 0.9666384534230308\n",
      "[33,   100] loss: 0.904833\n",
      "[33,   200] loss: 1.809665\n",
      "[33,   300] loss: 2.714498\n",
      "Epoch 33 validation loss: 0.9650052757490248\n",
      "[34,   100] loss: 0.904833\n",
      "[34,   200] loss: 1.809665\n",
      "[34,   300] loss: 2.714498\n",
      "Epoch 34 validation loss: 0.9633923778458248\n",
      "[35,   100] loss: 0.904833\n",
      "[35,   200] loss: 1.809665\n",
      "[35,   300] loss: 2.714498\n",
      "Epoch 35 validation loss: 0.9625439643859863\n",
      "[36,   100] loss: 0.904833\n",
      "[36,   200] loss: 1.809665\n",
      "[36,   300] loss: 2.714498\n",
      "Epoch 36 validation loss: 0.9605821342695327\n",
      "[37,   100] loss: 0.904833\n",
      "[37,   200] loss: 1.809665\n",
      "[37,   300] loss: 2.714498\n",
      "Epoch 37 validation loss: 0.957015872001648\n",
      "[38,   100] loss: 0.904833\n",
      "[38,   200] loss: 1.809665\n",
      "[38,   300] loss: 2.714498\n",
      "Epoch 38 validation loss: 0.9553760290145874\n",
      "[39,   100] loss: 0.904833\n",
      "[39,   200] loss: 1.809665\n",
      "[39,   300] loss: 2.714498\n",
      "Epoch 39 validation loss: 0.9560637426754784\n",
      "[40,   100] loss: 0.904833\n",
      "[40,   200] loss: 1.809665\n",
      "[40,   300] loss: 2.714498\n",
      "Epoch 40 validation loss: 0.9551159739494324\n",
      "[41,   100] loss: 0.904833\n",
      "[41,   200] loss: 1.809665\n",
      "[41,   300] loss: 2.714498\n",
      "Epoch 41 validation loss: 0.9513233285101633\n",
      "[42,   100] loss: 0.904833\n",
      "[42,   200] loss: 1.809665\n",
      "[42,   300] loss: 2.714498\n",
      "Epoch 42 validation loss: 0.9506471809886751\n",
      "[43,   100] loss: 0.904833\n",
      "[43,   200] loss: 1.809665\n",
      "[43,   300] loss: 2.714498\n",
      "Epoch 43 validation loss: 0.949400782585144\n",
      "[44,   100] loss: 0.904833\n",
      "[44,   200] loss: 1.809665\n",
      "[44,   300] loss: 2.714498\n",
      "Epoch 44 validation loss: 0.9499254207762461\n",
      "[45,   100] loss: 0.904833\n",
      "[45,   200] loss: 1.809665\n",
      "[45,   300] loss: 2.714498\n",
      "Epoch 45 validation loss: 0.9508332051928081\n",
      "[46,   100] loss: 0.904833\n",
      "[46,   200] loss: 1.809665\n",
      "[46,   300] loss: 2.714498\n",
      "Epoch 46 validation loss: 0.951412976734222\n",
      "[47,   100] loss: 0.904833\n",
      "[47,   200] loss: 1.809665\n",
      "[47,   300] loss: 2.714498\n",
      "Epoch 47 validation loss: 0.9554219264832754\n",
      "[48,   100] loss: 0.904833\n",
      "[48,   200] loss: 1.809665\n",
      "[48,   300] loss: 2.714498\n",
      "Epoch 48 validation loss: 0.9599791717907739\n",
      "[49,   100] loss: 0.904833\n",
      "[49,   200] loss: 1.809665\n",
      "[49,   300] loss: 2.714498\n",
      "Epoch 49 validation loss: 0.9584730276985775\n",
      "[50,   100] loss: 0.904833\n",
      "[50,   200] loss: 1.809665\n",
      "[50,   300] loss: 2.714498\n",
      "Epoch 50 validation loss: 0.9617491941603403\n",
      "[51,   100] loss: 0.904833\n",
      "[51,   200] loss: 1.809665\n",
      "[51,   300] loss: 2.714498\n",
      "Epoch 51 validation loss: 0.9659131169319153\n",
      "[52,   100] loss: 0.904833\n",
      "[52,   200] loss: 1.809665\n",
      "[52,   300] loss: 2.714498\n",
      "Epoch 52 validation loss: 0.9701138687512231\n",
      "[53,   100] loss: 0.904833\n",
      "[53,   200] loss: 1.809665\n",
      "[53,   300] loss: 2.714498\n",
      "Epoch 53 validation loss: 0.9740204830018301\n",
      "[54,   100] loss: 0.904833\n",
      "[54,   200] loss: 1.809665\n",
      "[54,   300] loss: 2.714498\n",
      "Epoch 54 validation loss: 0.9783290586774311\n",
      "[55,   100] loss: 0.904833\n",
      "[55,   200] loss: 1.809665\n",
      "[55,   300] loss: 2.714498\n",
      "Epoch 55 validation loss: 0.9831086406632076\n",
      "[56,   100] loss: 0.904833\n",
      "[56,   200] loss: 1.809665\n",
      "[56,   300] loss: 2.714498\n",
      "Epoch 56 validation loss: 0.9862698278729878\n",
      "[57,   100] loss: 0.904833\n",
      "[57,   200] loss: 1.809665\n",
      "[57,   300] loss: 2.714498\n",
      "Epoch 57 validation loss: 0.9916179785652767\n",
      "[58,   100] loss: 0.904833\n",
      "[58,   200] loss: 1.809665\n",
      "[58,   300] loss: 2.714498\n",
      "Epoch 58 validation loss: 0.9953216314315796\n",
      "[59,   100] loss: 0.904833\n",
      "[59,   200] loss: 1.809665\n",
      "[59,   300] loss: 2.714498\n",
      "Epoch 59 validation loss: 1.1968831940302773\n",
      "[60,   100] loss: 0.904833\n",
      "[60,   200] loss: 1.809665\n",
      "[60,   300] loss: 2.714498\n",
      "Epoch 60 validation loss: 1.2007486801298837\n",
      "[61,   100] loss: 0.904833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61,   200] loss: 1.809665\n",
      "[61,   300] loss: 2.714498\n",
      "Epoch 61 validation loss: 1.2038424071811495\n",
      "[62,   100] loss: 0.904833\n",
      "[62,   200] loss: 1.809665\n",
      "[62,   300] loss: 2.714498\n",
      "Epoch 62 validation loss: 1.2069060878148155\n",
      "[63,   100] loss: 0.904833\n",
      "[63,   200] loss: 1.809665\n",
      "[63,   300] loss: 2.714498\n",
      "Epoch 63 validation loss: 1.2096530293661452\n",
      "[64,   100] loss: 0.904833\n",
      "[64,   200] loss: 1.809665\n",
      "[64,   300] loss: 2.714498\n",
      "Epoch 64 validation loss: 1.2121490259019156\n",
      "[65,   100] loss: 0.904833\n",
      "[65,   200] loss: 1.809665\n",
      "[65,   300] loss: 2.714498\n",
      "Epoch 65 validation loss: 1.2144012451171875\n",
      "[66,   100] loss: 0.904833\n",
      "[66,   200] loss: 1.809665\n",
      "[66,   300] loss: 2.714498\n",
      "Epoch 66 validation loss: 1.2164340075992404\n",
      "[67,   100] loss: 0.904833\n",
      "[67,   200] loss: 1.809665\n",
      "[67,   300] loss: 2.714498\n",
      "Epoch 67 validation loss: 1.218374490737915\n",
      "[68,   100] loss: 0.904833\n",
      "[68,   200] loss: 1.809665\n",
      "[68,   300] loss: 2.714498\n",
      "Epoch 68 validation loss: 1.2200713138731698\n",
      "[69,   100] loss: 0.904833\n",
      "[69,   200] loss: 1.809665\n",
      "[69,   300] loss: 2.714498\n",
      "Epoch 69 validation loss: 1.2214670275884962\n",
      "[70,   100] loss: 0.904833\n",
      "[70,   200] loss: 1.809665\n",
      "[70,   300] loss: 2.714498\n",
      "Epoch 70 validation loss: 1.2226011715238057\n",
      "[71,   100] loss: 0.904833\n",
      "[71,   200] loss: 1.809665\n",
      "[71,   300] loss: 2.714498\n",
      "Epoch 71 validation loss: 1.2236165962521992\n",
      "[72,   100] loss: 0.904833\n",
      "[72,   200] loss: 1.809665\n",
      "[72,   300] loss: 2.714498\n",
      "Epoch 72 validation loss: 1.2243866882626973\n",
      "[73,   100] loss: 0.904833\n",
      "[73,   200] loss: 1.809665\n",
      "[73,   300] loss: 2.714498\n",
      "Epoch 73 validation loss: 1.2249693870544434\n",
      "[74,   100] loss: 0.904833\n",
      "[74,   200] loss: 1.809665\n",
      "[74,   300] loss: 2.714498\n",
      "Epoch 74 validation loss: 1.2253390607379733\n",
      "[75,   100] loss: 0.904833\n",
      "[75,   200] loss: 1.809665\n",
      "[75,   300] loss: 2.714498\n",
      "Epoch 75 validation loss: 1.2256040516353788\n",
      "[76,   100] loss: 0.904833\n",
      "[76,   200] loss: 1.809665\n",
      "[76,   300] loss: 2.714498\n",
      "Epoch 76 validation loss: 1.2260307111437359\n",
      "[77,   100] loss: 0.904833\n",
      "[77,   200] loss: 1.809665\n",
      "[77,   300] loss: 2.714498\n",
      "Epoch 77 validation loss: 1.2256959676742554\n",
      "[78,   100] loss: 0.904833\n",
      "[78,   200] loss: 1.809665\n",
      "[78,   300] loss: 2.714498\n",
      "Epoch 78 validation loss: 1.2256276588591317\n",
      "[79,   100] loss: 0.904833\n",
      "[79,   200] loss: 1.809665\n",
      "[79,   300] loss: 2.714498\n",
      "Epoch 79 validation loss: 1.2256134748458862\n",
      "[80,   100] loss: 0.904833\n",
      "[80,   200] loss: 1.809665\n",
      "[80,   300] loss: 2.714498\n",
      "Epoch 80 validation loss: 1.225662350654602\n",
      "[81,   100] loss: 0.904833\n",
      "[81,   200] loss: 1.809665\n",
      "[81,   300] loss: 2.714498\n",
      "Epoch 81 validation loss: 1.225738531067258\n",
      "[82,   100] loss: 0.904833\n",
      "[82,   200] loss: 1.809665\n",
      "[82,   300] loss: 2.714498\n",
      "Epoch 82 validation loss: 1.2255929746325054\n",
      "[83,   100] loss: 0.904833\n",
      "[83,   200] loss: 1.809665\n",
      "[83,   300] loss: 2.714498\n",
      "Epoch 83 validation loss: 1.225553279831296\n",
      "[84,   100] loss: 0.904833\n",
      "[84,   200] loss: 1.809665\n",
      "[84,   300] loss: 2.714498\n",
      "Epoch 84 validation loss: 1.2256834469144307\n",
      "[85,   100] loss: 0.904833\n",
      "[85,   200] loss: 1.809665\n",
      "[85,   300] loss: 2.714498\n",
      "Epoch 85 validation loss: 1.2257150544060602\n",
      "[86,   100] loss: 0.904833\n",
      "[86,   200] loss: 1.809665\n",
      "[86,   300] loss: 2.714498\n",
      "Epoch 86 validation loss: 1.2257608250966148\n",
      "[87,   100] loss: 0.904833\n",
      "[87,   200] loss: 1.809665\n",
      "[87,   300] loss: 2.714498\n",
      "Epoch 87 validation loss: 1.225912334427001\n",
      "[88,   100] loss: 0.904833\n",
      "[88,   200] loss: 1.809665\n",
      "[88,   300] loss: 2.714498\n",
      "Epoch 88 validation loss: 1.226028323173523\n",
      "[89,   100] loss: 0.904833\n",
      "[89,   200] loss: 1.809665\n",
      "[89,   300] loss: 2.714498\n",
      "Epoch 89 validation loss: 1.2260902003636436\n",
      "[90,   100] loss: 0.904833\n",
      "[90,   200] loss: 1.809665\n",
      "[90,   300] loss: 2.714498\n",
      "Epoch 90 validation loss: 1.2261587400285026\n",
      "[91,   100] loss: 0.904833\n",
      "[91,   200] loss: 1.809665\n",
      "[91,   300] loss: 2.714498\n",
      "Epoch 91 validation loss: 1.2263160962907096\n",
      "[92,   100] loss: 0.904833\n",
      "[92,   200] loss: 1.809665\n",
      "[92,   300] loss: 2.714498\n",
      "Epoch 92 validation loss: 1.2264682149130202\n",
      "[93,   100] loss: 0.904833\n",
      "[93,   200] loss: 1.809665\n",
      "[93,   300] loss: 2.714498\n",
      "Epoch 93 validation loss: 1.2265139893879966\n",
      "[94,   100] loss: 0.904833\n",
      "[94,   200] loss: 1.809665\n",
      "[94,   300] loss: 2.714498\n",
      "Epoch 94 validation loss: 1.2266592903742715\n",
      "[95,   100] loss: 0.904833\n",
      "[95,   200] loss: 1.809665\n",
      "[95,   300] loss: 2.714498\n",
      "Epoch 95 validation loss: 1.2268004417419434\n",
      "[96,   100] loss: 0.904833\n",
      "[96,   200] loss: 1.809665\n",
      "[96,   300] loss: 2.714498\n",
      "Epoch 96 validation loss: 1.423150660499694\n",
      "[97,   100] loss: 0.904833\n",
      "[97,   200] loss: 1.809665\n",
      "[97,   300] loss: 2.714498\n",
      "Epoch 97 validation loss: 1.4227508287581185\n",
      "[98,   100] loss: 0.904833\n",
      "[98,   200] loss: 1.809665\n",
      "[98,   300] loss: 2.714498\n",
      "Epoch 98 validation loss: 1.42270194727277\n",
      "[99,   100] loss: 0.904833\n",
      "[99,   200] loss: 1.809665\n",
      "[99,   300] loss: 2.714498\n",
      "Epoch 99 validation loss: 1.422432645918831\n",
      "[100,   100] loss: 0.904833\n",
      "[100,   200] loss: 1.809665\n",
      "[100,   300] loss: 2.714498\n",
      "Epoch 100 validation loss: 1.4220399894411602\n"
     ]
    }
   ],
   "source": [
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_to_words(X):\n",
    "    \"\"\"\n",
    "    X is of shape (batch, n_seq, max_word_length, vocab_size)\n",
    "    \"\"\"\n",
    "    array = X.data.numpy()\n",
    "    words =  np.ndarray((array.shape[0], array.shape[1]), dtype=object)\n",
    "    words.fill('')\n",
    "    #print(f'Words shape: {words.shape}')\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            for k in range(X.shape[2]):\n",
    "                if max(X[i,j,k,:]) == 1:\n",
    "                    words[i,j] += LETTERS[np.argmax(X[i,j,k,:])]\n",
    "                else:\n",
    "                    pass\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, args):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    report = ''\n",
    "    \n",
    "    # Training\n",
    "    correct_orders = 0\n",
    "    total_orders = 0\n",
    "    loader_len = len(test_loader)\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        X, Y, additional_dict = data\n",
    "        # Transfer to GPU\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        X, Y = X.to(device).float(), Y.to(device)\n",
    "        #X, Y = X.cuda().float(), Y.cuda()\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, pointers, hidden = model(X)\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, outputs.size()[-1])\n",
    "        #print(f'outputs: {outputs.size()}, Y: {Y.size()}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        if args.reader == 'words':\n",
    "            words = X_to_words(X.cpu())\n",
    "            #inds_x = np.tile(np.array(range(words.shape[0])), [words.shape[1], 1]).T\n",
    "            predicted_inds = pointers.cpu().data.numpy()\n",
    "            real_inds = Y.cpu().data.numpy()\n",
    "            for i in range(real_inds.shape[0]):\n",
    "                #print(f' Predicted Words order: {words[i, predicted_inds[i,:]]}')\n",
    "                report += f' Predicted Words order: {words[i, predicted_inds[i,:]]}\\n'\n",
    "                #print(f' Real Words order: {words[i, real_inds[i,:]]}\\n')\n",
    "                report += f' Real Words order: {words[i, real_inds[i,:]]}\\n'\n",
    "            \n",
    "        else :\n",
    "            #print(f'Predictions: {pointers}')\n",
    "            report += f'Predictions: {pointers}\\n'\n",
    "            #print(f'Real orders: {Y}')\n",
    "            report+= f'Real orders: {Y}\\n'\n",
    "            \n",
    "        for _ in range(pointers.size(0)):\n",
    "            total_orders += 1\n",
    "            if Y[_,:].equal( pointers[_,:]):\n",
    "                correct_orders +=1\n",
    "                \n",
    "    #print(f'Fraction of perfectly sorted sets: {correct_orders/total_orders}')\n",
    "    report += f'Fraction of perfectly sorted sets: {correct_orders/total_orders}\\n'\n",
    "    \n",
    "    \n",
    "    \n",
    "    return correct_orders/total_orders, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if torch.cuda.is_available():\n",
    "        args.USE_CUDA = True\n",
    "        print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "    else:\n",
    "        args.USE_CUDA = False\n",
    "        \n",
    "        \n",
    "    \n",
    "    with open(args.pickle_file, 'rb') as f:\n",
    "        dict_data = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    #runs = glob(args.saveprefix+'/*')\n",
    "    #it = len(runs) + 1\n",
    "    #writer = SummaryWriter(os.path.join(args.tensorboard_saveprefix, str(it)))\n",
    "    #writer.add_text('Metadata', 'Run {} metadata :\\n{}'.format(it, args,))\n",
    "    \n",
    "    dataset_class = DATASET_CLASSES[args.reader]\n",
    "    \n",
    "    test_ds = dataset_class(dict_data['test'])\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=args.batch_size, shuffle=True,\n",
    "            num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    \n",
    "    model = create_model(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.USE_CUDA:\n",
    "        device = torch.cuda.current_device()\n",
    "        #model.cuda()\n",
    "        device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device)\n",
    "        net = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "    return test(test_loader, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU, 4 devices.\n"
     ]
    }
   ],
   "source": [
    "DATASET_CLASSES = {'linear': DigitsDataset, 'words': WordsDataset, 'videos': VideosDataset}\n",
    "LETTERS = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_18374_3937_5_2019-06-18_11:45:26.327081.pkl' \n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/video_reordering_unpooled.pkl' \n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_10_2019-08-07_12:00:48.139281.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_5_2019-07-27_14:05:17.582365.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_15_2019-08-12_20:04:33.267868.pkl'\n",
    "#PICKLE_FILE = '../pickles/digits_reordering_10000_2000_30_2019-08-12_21:11:38.404306.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_5_2019-07-28_12:14:08.250888.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_10_2019-08-07_15:04:50.672650.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_15_2019-08-13_15:38:03.834675.pkl'\n",
    "#PICKLE_FILE = '../pickles/words_reordering_10000_2000_30_2019-08-13_18:02:03.381022.pkl'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_5.pkl'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_10.pkl'\n",
    "#PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_15.pkl'\n",
    "PICKLE_FILE = '../../s3-drive/set_to_sequence/word_reordering_from_dict_n_set_30.pkl'\n",
    "ID = 'ACD464DB'\n",
    "RESUME_ID = 'E9153422'\n",
    "EPOCH = 100\n",
    "RESUME = f'../checkpoints/words/{RESUME_ID}/ep_{EPOCH}_map_inf_latest.pth.tar'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "READ_HIDDEN_DIMS = [32]\n",
    "WRITE_HIDDEN_DIM = 32\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-6\n",
    "MOMENTUM = .9\n",
    "NESTEROV = False\n",
    "LSTM_STEPS = 20\n",
    "READER = 'words'\n",
    "INPUT_DIM = 26\n",
    "DROPOUT = 0.2\n",
    "WORKERS = 4\n",
    "PRINT_OFFSET = 100\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "else:\n",
    "    USE_CUDA = False\n",
    "\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "ARGS =parser.parse_args(args=[])\n",
    "ARGS.pickle_file = PICKLE_FILE\n",
    "ARGS.batch_size = BATCH_SIZE\n",
    "ARGS.read_hidden_dims = READ_HIDDEN_DIMS\n",
    "ARGS.write_hidden_dim = WRITE_HIDDEN_DIM\n",
    "ARGS.lr = LR\n",
    "ARGS.weight_decay = WEIGHT_DECAY\n",
    "ARGS.momentum = MOMENTUM\n",
    "ARGS.nesterov = NESTEROV\n",
    "ARGS.epochs = EPOCHS\n",
    "ARGS.lstm_steps = LSTM_STEPS\n",
    "ARGS.input_dim = INPUT_DIM\n",
    "ARGS.reader = READER\n",
    "ARGS.dropout = DROPOUT\n",
    "ARGS.workers = WORKERS\n",
    "ARGS.resume =RESUME\n",
    "ARGS.print_offset = PRINT_OFFSET\n",
    "ARGS.id = ID\n",
    "ARGS.resume = RESUME\n",
    "ARGS.USE_CUDA = USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture cap --no-stderr\n",
    "#main(ARGS)\n",
    "#print(create_model(ARGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'../evals/words/{ID}/ep_{EPOCH}.txt', 'w') as f:\n",
    "#   f.write(cap.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for epoch: 0\n",
      "\n",
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "=> no checkpoint found at '../checkpoints/words/E9153422/ep_0_map_inf_latest.pth.tar'\n",
      "0.0\n",
      "Accuracy for epoch: 1\n",
      "\n",
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "=> loading checkpoint '../checkpoints/words/E9153422/ep_1_map_inf_latest.pth.tar'\n",
      "0.0\n",
      "Accuracy for epoch: 5\n",
      "\n",
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "=> loading checkpoint '../checkpoints/words/E9153422/ep_5_map_inf_latest.pth.tar'\n",
      "0.0\n",
      "Accuracy for epoch: 10\n",
      "\n",
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "=> loading checkpoint '../checkpoints/words/E9153422/ep_10_map_inf_latest.pth.tar'\n",
      "0.0\n",
      "Accuracy for epoch: 20\n",
      "\n",
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "=> loading checkpoint '../checkpoints/words/E9153422/ep_20_map_inf_latest.pth.tar'\n",
      "0.0\n",
      "Accuracy for epoch: 50\n",
      "\n",
      "Using GPU, 4 devices.\n",
      "=> creating model\n",
      "=> loading checkpoint '../checkpoints/words/E9153422/ep_50_map_inf_latest.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "EPOCHS = 100\n",
    "epoch_list = [0,1,5,10,20,50,100]\n",
    "for epoch in [x for x in epoch_list if x <= EPOCHS]:\n",
    "    \n",
    "    RESUME = f'../checkpoints/words/{RESUME_ID}/ep_{epoch}_map_inf_latest.pth.tar'\n",
    "    ARGS.epochs = epoch\n",
    "    ARGS.resume = RESUME\n",
    "    #print('ARGS: ', ARGS)\n",
    "    s = f'Accuracy for epoch: {ARGS.epochs}\\n'\n",
    "    print(s)\n",
    "    accuracy, report  = main(ARGS)\n",
    "    s += report \n",
    "    with open(f'../evals/words/{ARGS.id}/ep_{ARGS.epochs}.txt', 'w') as f:\n",
    "        f.write(s)\n",
    "    print(accuracy)\n",
    "    accuracies.append(accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Fraction of perfectly sorted sets')\n",
    "plt.savefig(f'../evals/words/{ARGS.id}/accuracy.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(rpw.named_parameters())\n",
    "for for name, param in rpw.named_parameters():\n",
    "    if param.requires_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "weights_indices = {}\n",
    "l = list(rpw.named_parameters())\n",
    "for name, param in l:\n",
    "    if param.requires_grad:\n",
    "        size = list(param.data.flatten().size())[0]\n",
    "        weights_indices[name] = random.sample(range(size), 5)\n",
    "weights_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weights(weights_indices, parameters, writer)\n",
    "    weights_data = {}\n",
    "    for name, param in parameters:\n",
    "        if param.requires_grad:\n",
    "            indices = weights_indices[name]\n",
    "            for idx in indices:\n",
    "                weights_data[f'{name}.{idx}'] = params.data/flatten()[idx]\n",
    "    writer.add_scalars('data/weights', weights_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ''\n",
    "a+= 's'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.uniform(size=(10000, 5))\n",
    "Y_train = np.sort(X, axis=1)\n",
    "X_val = np.random.uniform(size=(10000, 5))\n",
    "Y_val = np.sort(X, axis=1)\n",
    "Y_train[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {'attributes': None, 'split':{'train': [], 'val': []}}\n",
    "for i in range(X_train.shape[0]):\n",
    "    dict_data['split']['train'].append((X_train[i, :], Y_train[i,:]))\n",
    "    dict_data['split']['val'].append((X_val[i, :], Y_val[i,:]))\n",
    "dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (virtualenv_set2seq)",
   "language": "python",
   "name": "virtualenv_set2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

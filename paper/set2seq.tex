\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Set to Sequence}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Introduction}

NeurIPS requires electronic submissions.  The electronic submission site is
\begin{center}
  \url{https://cmt.research.microsoft.com/NeurIPS2019/}
\end{center}



\section{Related Work}
\label{sec:rw}


\subsubsection*{Acknowledgments}

NSF Award number ***

\section*{References}

\subsection{\cite{vinyals2015order: Order matters}}

\begin{itemize}
\item Multiple examples of seq2seq problems where the sequence can not be understood as a set because the order of the elements matter as it significantly impact performance.

\item read, process, write paradigm: an encoder that represent each input as a memory vector $m_i$. a processor inspired by attention mechanism - It's an LSTM without inputs or outputs performing T steps of computation over the memories $m_i$-. a decoder that is a pointer network - uses the attention mechanism softmax coefficients as probability of outputting each input at each step.

\item The output of the sequence matters: even if we want to model the joint probablity of a set using bayes rules, some ordering of the conditionning might be easier to learn than other. So it is useful to allow the algorithm to learn the best ordering while training. 
\end{itemize}

\subsection{\cite{cui2018deep: Deep Attentive Sentence Ordering Network}}

\begin{itemize}

\item Reordering the senetences of a paragraph. the set is a set of sentences. Heavily inspired by the transformer (attention is all you need architechture), all the sentences are encoded in vector form. Then multiple steps of multihead attention are applied to make a feature represention of a of the entire paragraph. 
\item nltk sentence tokenizer used

\end{itemize}

% TODO: Import bibtex
\bibliography{references} 
\bibliographystyle{humannat}

\end{document}
